<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sam Herold">
<meta name="dcterms.date" content="2024-05-14">
<meta name="description" content="Overview of the current state of image denoising. Investigating traditional and deep learning methods.">

<title>Notebook: Analysis of Unsupervised Denoising Methods – Sam Herold</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Sam Herold</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract"><strong>Abstract</strong></a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><strong>Background</strong></a></li>
  <li><a href="#dataset-notes" id="toc-dataset-notes" class="nav-link" data-scroll-target="#dataset-notes"><strong>Dataset Notes</strong></a></li>
  <li><a href="#load-dependencies-load-data-and-create-helper-methods" id="toc-load-dependencies-load-data-and-create-helper-methods" class="nav-link" data-scroll-target="#load-dependencies-load-data-and-create-helper-methods"><strong>Load dependencies, load data, and create helper methods</strong></a></li>
  </ul></li>
  <li><a href="#investigating-denoisers" id="toc-investigating-denoisers" class="nav-link" data-scroll-target="#investigating-denoisers">Investigating Denoisers</a>
  <ul class="collapse">
  <li><a href="#linear-filtering" id="toc-linear-filtering" class="nav-link" data-scroll-target="#linear-filtering"><strong>Linear Filtering</strong></a></li>
  <li><a href="#other-filters" id="toc-other-filters" class="nav-link" data-scroll-target="#other-filters"><strong>Other Filters</strong></a></li>
  <li><a href="#bm3d" id="toc-bm3d" class="nav-link" data-scroll-target="#bm3d"><strong>BM3D</strong></a></li>
  <li><a href="#noise2noise" id="toc-noise2noise" class="nav-link" data-scroll-target="#noise2noise"><strong>Noise2Noise</strong></a></li>
  <li><a href="#noise2void" id="toc-noise2void" class="nav-link" data-scroll-target="#noise2void"><strong>Noise2Void</strong></a></li>
  <li><a href="#some-other-noise2noise-variants" id="toc-some-other-noise2noise-variants" class="nav-link" data-scroll-target="#some-other-noise2noise-variants"><strong>Some other Noise2Noise Variants</strong></a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notebook: Analysis of Unsupervised Denoising Methods</h1>
</div>

<div>
  <div class="description">
    Overview of the current state of image denoising. Investigating traditional and deep learning methods.
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://samhero16.github.io/">Sam Herold</a> <a href="https://orcid.org/0000-0002-5300-3075" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 14, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract"><strong>Abstract</strong></h3>
<p>Image denoising is a classic problem in computer vision and image processing. The goal is simple: to remove noise from a noisy image and recover the underlying image. The problem statement is often written as x ̂=x+n, where x ̂ is the observed image, n is additive noise and x is the ground truth image that we hope to recover. Often times, we do not have access to any x (ground truth) images, making the problem unsupervised. There are many ways people have attempted to do unsupervised denoising, and I will be exploring the most popular in this paper.</p>
</section>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background"><strong>Background</strong></h3>
<p>Noisy images are very prevalent in engineering, biology, and many other areas, making denoising very important and heavily researched. I was introduced to the world of image denoising during an undergraduate research project in the Department of Electrical Engineering, with Dr.&nbsp;Jesse Wilson and his lab. Their new laser scan technology produces biomedical images that contain a significant amount of noise. Their images will always contain noise (due to the electronics, the nature of light, laser exposure limits, etc.), meaning there is no access to any ground truth images ever. This led to questions about unsupervised denoising options.</p>
<p>The labs current approach is stack averaging (taking multiple images of the same thing, then averaging them). This method is consistent, but costly, slow, and can damage the sample. This begged for an instant denoising method. My research task was to investigate the deep learning Noise2Noise framework. I will discuss this method later on in this paper.</p>
<p>My goal with this paper is to investigate unsupervised denoisers, both traditional and in deep learning, and try them on some biomedical data from the lab.</p>
</section>
<section id="dataset-notes" class="level3">
<h3 class="anchored" data-anchor-id="dataset-notes"><strong>Dataset Notes</strong></h3>
<p>I will be trying some of these methods on the noisy images collected from Dr.&nbsp;Jesse Wilson’s lab. These are collected in stacks (collections of image with the same underlying image, but with different noise instances). They are 128x128 grayscale images, who’s pixel values are centered around zero, not between 0 and 255. We also believe the noise on the images is additive and gaussian. The noise level is very high on these images.</p>
<p>For understanding the output of the different denoisers, we will compare their outpurs to the stack averages, both visually and using MSE. The stack average is certainly not the ground truth, but the best that we can do and shows us at least some of the true underlying structure. The MSE definetely does not make or break the ability of the denoiser, it is just gives us some kind of slight reference.</p>
</section>
<section id="load-dependencies-load-data-and-create-helper-methods" class="level3">
<h3 class="anchored" data-anchor-id="load-dependencies-load-data-and-create-helper-methods"><strong>Load dependencies, load data, and create helper methods</strong></h3>
<div id="9b017568" class="cell" data-outputid="b40d31a7-4899-48cf-ca7d-eef34ae89e3c" data-execution_count="9">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#dependencies</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> ndimage</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.signal <span class="im">import</span> wiener</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam, lr_scheduler</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> n2v</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> n2v.models <span class="im">import</span> N2VConfig, N2V</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> csbdeep.utils <span class="im">import</span> plot_history</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> n2v.utils.n2v_utils <span class="im">import</span> manipulate_val_data</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> n2v.internals.N2V_DataGenerator <span class="im">import</span> N2V_DataGenerator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c41e872e" class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#load</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> []</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> [] <span class="co">#averages</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"/content/gdrive/MyDrive/CS445Final/2023-08-30-FullDataSet"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>XFull <span class="op">=</span> []</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#loop through folder containing tifs</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> image_class <span class="kw">in</span> os.listdir(path):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>      average <span class="op">=</span> np.zeros([<span class="dv">128</span>,<span class="dv">128</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>      sameBaseImages <span class="op">=</span> []</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>      <span class="co">#open tif file</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>      image_path <span class="op">=</span> os.path.join(path, image_class)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>      tif <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>      X.append(np.array(tif))<span class="co">#just add one from each stack</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>      <span class="co">#loads images for stack average</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tif.n_frames):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>          tif.seek(i)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>          img <span class="op">=</span> np.array(tif)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>          XFull.append(img)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>          average <span class="op">+=</span> img</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>      a.append(average<span class="op">/</span>tif.n_frames)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="NWhn-DrxQW1m" class="cell" data-outputid="d7091c02-7ec1-4bc0-f172-f1f383b88e8d" data-execution_count="88">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(X[<span class="dv">0</span>])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Noisy'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Stack Average'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(a[<span class="dv">0</span>])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Example of noisy image and its corresponding stack average, the closest to ground truth that we can get.  '</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Example of noisy image and its corresponding stack average, the closest to ground truth that we can get.  </code></pre>
</div>
</div>
<div id="ec22998f" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(original, denoised,average, title):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"MSE (denoised vs average): "</span> , <span class="bu">round</span>(np.mean((denoised <span class="op">-</span> average)<span class="op">**</span><span class="dv">2</span>),<span class="dv">5</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">131</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(original)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Original Image'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">132</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    plt.imshow(denoised)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">133</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    plt.imshow(average)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Average Stack'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
<hr>
</section>
</section>
<section id="investigating-denoisers" class="level1">
<h1>Investigating Denoisers</h1>
<section id="linear-filtering" class="level3">
<h3 class="anchored" data-anchor-id="linear-filtering"><strong>Linear Filtering</strong></h3>
<p>[Fan et al., 2019]</p>
<p>Linear filtering is the most straightforward way to denoise. This method involves sliding a moving window or kernel over each pixel in the image. Some operation is done on that window (i.e.&nbsp;mean, median), and the center pixel is replaced with the outcome of that operation. All of these filters have hyperparameters that you can tune for your images. In the following examples, I tuned the hyperparamters by hand until the outputs were as optimal as they could be.</p>
<p><em>Mean Filter</em>: Take the mean of every surrounding pixel in the kernel and replace the center pixel with the mean.</p>
<p><em>Median Filter</em>: Take the median of every surrounding pixel in the kernel and replace the center pixel with the mean.</p>
<p><em>Gaussian Filter</em>: Uses a gaussian kernel for convolution. Uses a wieghted average of the kernel that is determined by the gaussian distribution. Smooths the image well (think of gaussian blur from photoshop).</p>
<p><em>Weiner Filter</em>: The wiener filter uses the frequency domain. It is complicated and uses Fourier Transforms.</p>
<section id="mean-filtering" class="level4">
<h4 class="anchored" data-anchor-id="mean-filtering">Mean Filtering</h4>
<div id="9f0929a0" class="cell" data-outputid="81d23425-3aeb-4026-b9e6-26069b1a47bb" data-execution_count="89">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.uniform_filter(X[<span class="dv">0</span>], size<span class="op">=</span>kernel)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], filtered, a[<span class="dv">0</span>],  <span class="st">'Mean Filtered Image'</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.uniform_filter(X[<span class="dv">1</span>], size<span class="op">=</span>kernel)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], filtered, a[<span class="dv">1</span>], <span class="st">'Mean Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00546</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00617</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-6-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="median-filtering" class="level4">
<h4 class="anchored" data-anchor-id="median-filtering">Median Filtering</h4>
<div id="09d3caff" class="cell" data-outputid="0d51ca87-fc30-4ff7-8155-86df5a0e9e0e" data-execution_count="50">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.median_filter(X[<span class="dv">0</span>], size<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], filtered, a[<span class="dv">0</span>],<span class="st">'Median Filtered Image'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.median_filter(X[<span class="dv">1</span>], size<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], filtered, a[<span class="dv">1</span>],<span class="st">'Median Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00756</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00852</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-7-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gaussian-filtering" class="level4">
<h4 class="anchored" data-anchor-id="gaussian-filtering">Gaussian Filtering</h4>
<div id="1625546b" class="cell" data-outputid="380659da-e39f-4c89-faaa-ac376d76945a" data-execution_count="82">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.gaussian_filter(X[<span class="dv">0</span>], sigma <span class="op">=</span> <span class="fl">1.5</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], filtered, a[<span class="dv">0</span>],<span class="st">'Gaussian Filtered Image'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> ndimage.gaussian_filter(X[<span class="dv">1</span>], sigma <span class="op">=</span> <span class="fl">1.5</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], filtered, a[<span class="dv">1</span>],<span class="st">'Gaussian Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00504</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00579</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-8-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="wiener-filtering" class="level4">
<h4 class="anchored" data-anchor-id="wiener-filtering">Wiener Filtering</h4>
<div id="596f82c2" class="cell" data-outputid="f5b93728-e930-432b-ae44-5df118034adf" data-execution_count="55">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> wiener(X[<span class="dv">0</span>],noise <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], filtered,a[<span class="dv">0</span>], <span class="st">'Wiener Filtered Image'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> wiener(X[<span class="dv">1</span>],noise <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], filtered, a[<span class="dv">1</span>],<span class="st">'Wiener Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00765</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  0.00858</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-9-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="pros" class="level4">
<h4 class="anchored" data-anchor-id="pros">Pros:</h4>
<ul>
<li>Simple to understand.</li>
<li>Simple to implement.</li>
<li>Only need one image.</li>
<li>Sensitive to hyperparameters.</li>
<li>Versatile.</li>
</ul>
</section>
<section id="cons" class="level4">
<h4 class="anchored" data-anchor-id="cons">Cons:</h4>
<ul>
<li>Poor performance.</li>
<li>Only uses local information.</li>
<li>Only can use one image even if others are available.</li>
<li>Bad with edges.</li>
</ul>
<p>The gaussian filter looks very promising. I will comment more on this in the conclusion.</p>
<hr>
<hr>
</section>
</section>
<section id="other-filters" class="level3">
<h3 class="anchored" data-anchor-id="other-filters"><strong>Other Filters</strong></h3>
<p>[Fan et al., 2019]</p>
<p>These methods up the complexity and use information from the whole image. This increases performance and edge preservation. Also, these methods require that I put the images between 1 and 255. This isnt so much a downside, but rather an inconvienience as it changes the scale of the MSE.</p>
<p><em>Non-Local Means</em>: Uses patches of similar makeup from other parts of the image for averaging a window. This average is weighted by how similar the image patches are.</p>
<p><em>Bilateral Filtering</em>: Uses a weighted average where spatial distance and pixel intensity are considered in weighting.</p>
<section id="non-local-means" class="level4">
<h4 class="anchored" data-anchor-id="non-local-means">Non-Local Means</h4>
<div id="bafacd62" class="cell" data-outputid="a469735e-3aef-43ec-9295-cfade5c93022" data-execution_count="57">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> ((X[<span class="dv">0</span>]  <span class="op">-</span> X[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">/</span> (X[<span class="dv">0</span>] .<span class="bu">max</span>() <span class="op">-</span> X[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8) <span class="co">#we need to normalize for cv2 to be able to function</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">0</span>]  <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">0</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8) <span class="co"># also need to do this for averages</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>NL <span class="op">=</span> cv2.fastNlMeansDenoising(normalized, <span class="va">None</span>, h<span class="op">=</span><span class="dv">18</span>, templateWindowSize<span class="op">=</span><span class="dv">9</span>, searchWindowSize<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], NL, ave, <span class="st">'Wiener Filtered Image'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> ((X[<span class="dv">1</span>]  <span class="op">-</span> X[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">/</span> (X[<span class="dv">1</span>] .<span class="bu">max</span>() <span class="op">-</span> X[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">1</span>]  <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">1</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>NL <span class="op">=</span> cv2.fastNlMeansDenoising(normalized, <span class="va">None</span>, h<span class="op">=</span><span class="dv">18</span>, templateWindowSize<span class="op">=</span><span class="dv">9</span>, searchWindowSize<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], NL, ave,<span class="st">'Wiener Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  97.04028</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  89.83875</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-10-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="bilateral-filtering" class="level4">
<h4 class="anchored" data-anchor-id="bilateral-filtering">Bilateral filtering</h4>
<div id="216b0429" class="cell" data-outputid="f9d36647-b722-4df4-e7ed-83ee9fdaa1ee" data-execution_count="58">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> ((X[<span class="dv">0</span>]  <span class="op">-</span> X[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">/</span> (X[<span class="dv">0</span>] .<span class="bu">max</span>() <span class="op">-</span> X[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>bilateral <span class="op">=</span> cv2.bilateralFilter(normalized, d<span class="op">=</span><span class="dv">4</span>, sigmaColor<span class="op">=</span><span class="dv">75</span>, sigmaSpace<span class="op">=</span><span class="dv">75</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">0</span>]  <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">0</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], bilateral, ave,<span class="st">'Wiener Filtered Image'</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> ((X[<span class="dv">1</span>]  <span class="op">-</span> X[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">/</span> (X[<span class="dv">1</span>] .<span class="bu">max</span>() <span class="op">-</span> X[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>bilateral <span class="op">=</span> cv2.bilateralFilter(normalized, d<span class="op">=</span><span class="dv">6</span>, sigmaColor<span class="op">=</span><span class="dv">75</span>, sigmaSpace<span class="op">=</span><span class="dv">75</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">1</span>]  <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">1</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>], bilateral, ave, <span class="st">'Wiener Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  95.36255</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  87.04791</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="pros-1" class="level4">
<h4 class="anchored" data-anchor-id="pros-1">Pros:</h4>
<ul>
<li>Simple to implement.</li>
<li>Increased complexity over simple filtering.</li>
<li>Require only one image.</li>
<li>Versatile.</li>
<li>Can pick up on detail.</li>
</ul>
</section>
<section id="cons-1" class="level4">
<h4 class="anchored" data-anchor-id="cons-1">Cons:</h4>
<ul>
<li>Decently complex to understand.</li>
<li>Very sensitive to its parameters.</li>
<li>Struggles with edge preservation.</li>
<li>Might create blurring/artifacts.</li>
<li>Definetly does not smooth the image.</li>
</ul>
<hr>
<hr>
</section>
</section>
<section id="bm3d" class="level3">
<h3 class="anchored" data-anchor-id="bm3d"><strong>BM3D</strong></h3>
<p>‘Block Matching 3D Filtering’ is a complex, state of the art denoising technique. This technique builds upon non-local means, in the sense that the algorithm groups similar patches using a block matching algorithm. These groups are then filtered an aggregated to produce the final image. BM3D is widely recognized to be one of the best options for denoising. It is easily avaible in the ‘bm3d’ package:</p>
<div id="cYl_Oln8KFEv" class="cell" data-outputid="a59a9075-6edc-4c45-aebf-a93a3ddb1c7f" data-execution_count="6">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install bm3d<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting bm3d
  Downloading bm3d-4.0.1-py3-none-any.whl (10.0 kB)
Collecting bm4d&gt;=4.2.3 (from bm3d)
  Downloading bm4d-4.2.3-py3-none-any.whl (1.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 5.8 MB/s eta 0:00:00
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bm3d) (1.25.2)
Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bm3d) (1.11.4)
Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from bm4d&gt;=4.2.3-&gt;bm3d) (1.6.0)
Installing collected packages: bm4d, bm3d
Successfully installed bm3d-4.0.1 bm4d-4.2.3</code></pre>
</div>
</div>
<div id="3qIyeBfamrHE" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> bm3d</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> io, img_as_float32</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The main hyperparameter for bm3d is sigma: this is supposed to be the standard deviation of the noise. I could not figure out how to find this with no ground truth, so I tried many different sigmas to see which one had the visually best result. The resulting sigma was 45.</p>
<div id="lolF_1valpbB" class="cell" data-outputid="05b684a0-88bd-450c-937a-20e7c78d6179" data-execution_count="85">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">45</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> io.imread(<span class="st">'/content/gdrive/MyDrive/CS445Final/Png2/20230830_15_40_14.png'</span>, as_gray <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>bm3dout <span class="op">=</span> bm3d.bm3d(image, sigma_psd <span class="op">=</span> s , stage_arg <span class="op">=</span> bm3d.BM3DStages.ALL_STAGES)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">0</span>]  <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">0</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">0</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>],bm3dout, ave, <span class="st">'BM3D Filtered Image'</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> io.imread(<span class="st">'/content/gdrive/MyDrive/CS445Final/Png2/20230830_16_01_28.png'</span>, as_gray <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>bm3dout <span class="op">=</span> bm3d.bm3d(image, sigma_psd <span class="op">=</span> s, stage_arg <span class="op">=</span> bm3d.BM3DStages.ALL_STAGES)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">1</span>]  <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">1</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">1</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">1</span>],bm3dout,ave, <span class="st">'BM3D Filtered Image'</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> io.imread(<span class="st">'/content/gdrive/MyDrive/CS445Final/Png2/20230830_16_01_54.png'</span>, as_gray <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>bm3dout <span class="op">=</span> bm3d.bm3d(image, sigma_psd <span class="op">=</span> s, stage_arg <span class="op">=</span> bm3d.BM3DStages.ALL_STAGES)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>ave <span class="op">=</span> ((a[<span class="dv">2</span>]  <span class="op">-</span> a[<span class="dv">2</span>] .<span class="bu">min</span>()) <span class="op">/</span> (a[<span class="dv">2</span>] .<span class="bu">max</span>() <span class="op">-</span> a[<span class="dv">2</span>] .<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">2</span>],bm3dout, ave, <span class="st">'BM3D Filtered Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  2422.69757</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  1621.71947</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-14-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE (denoised vs average):  1216.8787</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-14-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>BM3D does not seem to be the most effective for this particular problem. I think visually it gives some decent results, but wierdly the MSE is really large. It definetely makes the image smooth and ‘not noisy’, but it misses a ton of detail in exchange. Everything I read says that it is probably one if not the best option for denoising a single image, so I do not count it out.</p>
<p>Pros: - Good performance. - Easy to implement and use. - Uses non-local imformation. - Versatile and robust. - Only requires on image.</p>
<p>Cons: - Sensitive to hyperparameters.. - Can take a long time for some images. - May miss out on some fine detail structure. - Mainly good at denoising additive gaussian. - Not available on Mac Silicon (this was frustrating).</p>
<hr>
<hr>
</section>
<section id="noise2noise" class="level3">
<h3 class="anchored" data-anchor-id="noise2noise"><strong>Noise2Noise</strong></h3>
<p>[Lehtinen et al., 2018]</p>
<p>It is seemingly obvious that a neural network could learn to denoise images if trained on noisy input images with clean targets (the network learns how to go from noisy to clean). Call this Noise2Clean. However, it is often difficult, costly or impossible to get clean image targets. Noise2Noise is a deep learning framework that uses only noisy images to denoise images, and rivals the performance of the aforementioned noisy to clean method. This framework was proposed in NVidias 2018 paper, “Noise2Noise: Learning Image Restoration without Clean Data.”</p>
<p>Noise2Noise uses noisy images as both the model input and the target. Through an interesting realization in math, this is essentially the same as using a clean image target. The model will try to predict the noisy target and fail. The best that it can do (while still reducing training loss) is predict the underlying, clean image.</p>
<p>Another way to think about it is with gradients. The gradient does not point exactly to the clean image, but rather many gradients point to images that average to the underlying clean images. In other words, the average gradient points to the unobserved clean image. In theory, the model will converge to the expected value of the noisy image.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Oreaq8x.png" class="img-fluid figure-img"></p>
<figcaption>image-2.png</figcaption>
</figure>
</div>
<p>[Lehtinen et al., 2018]</p>
<p>There are some serious benefits to using Noise2Noise. If you have noisy image pairs, this will likely give the best denoising that you can ask for. Unlike the denoisers seen before this, this model can use information from multiple images. Just based off that, it makes sense that this framework will outperform algorithms that use just one image.</p>
<p>Noise2Noise uses a U-NET architecture. Essentially, U-NET is an image to image convolutional neural network. The inputs and outputs are both images.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_f7YOaE4TWubwaFF7Z1fzNw.png" class="img-fluid figure-img"></p>
<figcaption>image-3.png</figcaption>
</figure>
</div>
<p>[Ronneberger et al., 2015]</p>
<p>As you can see, UNET gets its name from its shape. In essence, it squeezes images down (encoding) to extract information, then builds them back up (decoding) to predict the target. The encoder section is a series of convolutional and max-pooling layers, and the decoder is a series of up sampling layers. Another important aspect of UNET is its skip connections (arrows across the top). These basically give the decoder some information about the structure at different stages of encoding that it can use if needed. Learning about upsampling, maxpooling and skip connections were all new to me and are super cool.</p>
<ul>
<li>Encoder
<ul>
<li>Convolutional layer(s) followed by max pooling layer</li>
<li>Extracts and separates information</li>
</ul></li>
<li>Decoder
<ul>
<li>Convolutional layer and transpose convolutional layer.</li>
<li>Up samples feature maps and combines them to predict the final image.</li>
</ul></li>
<li>Skip Connections
<ul>
<li>Connect layers of the the encoder to layers of the decoder</li>
<li>Provide information that may be useful information from earlier layers.</li>
<li>This makes extra sense for Noise2Noise, as the target image is very similar to the input image.</li>
</ul></li>
</ul>
<p>Below is an implementation of the UNET (in pytorch) used in Noise2Noise. This was taken from the Noise2Noise github page. This is a fairly complex model as you can see.</p>
<div id="d059b9f8" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Unet pulled from Noise2Noise Github Repo [Lehtinen et al., 2018], [Ronneberger et al., 2015]</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UNet(nn.Module):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Custom U-Net architecture for Noise2Noise (see Appendix, Table 2)."""</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initializes U-Net."""</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(UNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: enc_conv0, enc_conv1, pool1</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, <span class="dv">48</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">48</span>, <span class="dv">48</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>))</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: enc_conv(i), pool(i); i=2..5</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">48</span>, <span class="dv">48</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>))</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: enc_conv6, upsample5</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block3 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">48</span>, <span class="dv">48</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">48</span>, <span class="dv">48</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.Upsample(scale_factor=2, mode='nearest'))</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: dec_conv5a, dec_conv5b, upsample4</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block4 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.Upsample(scale_factor=2, mode='nearest'))</span></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: dec_deconv(i)a, dec_deconv(i)b, upsample(i-1); i=4..2</span></span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block5 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">144</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">96</span>, <span class="dv">96</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.Upsample(scale_factor=2, mode='nearest'))</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layers: dec_conv1a, dec_conv1b, dec_conv1c,</span></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._block6 <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">96</span> <span class="op">+</span> in_channels, <span class="dv">64</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, out_channels, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(<span class="fl">0.1</span>))</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights</span></span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>):</span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initializes weights using He et al. (2015)."""</span></span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.modules():</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.ConvTranspose2d) <span class="kw">or</span> <span class="bu">isinstance</span>(m, nn.Conv2d):</span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>                nn.init.kaiming_normal_(m.weight.data)</span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>                m.bias.data.zero_()</span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Through encoder, then decoder by adding U-skip connections. """</span></span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encoder</span></span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a>        pool1 <span class="op">=</span> <span class="va">self</span>._block1(x)</span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a>        pool2 <span class="op">=</span> <span class="va">self</span>._block2(pool1)</span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a>        pool3 <span class="op">=</span> <span class="va">self</span>._block2(pool2)</span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a>        pool4 <span class="op">=</span> <span class="va">self</span>._block2(pool3)</span>
<span id="cb31-79"><a href="#cb31-79" aria-hidden="true" tabindex="-1"></a>        pool5 <span class="op">=</span> <span class="va">self</span>._block2(pool4)</span>
<span id="cb31-80"><a href="#cb31-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-81"><a href="#cb31-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Decoder</span></span>
<span id="cb31-82"><a href="#cb31-82" aria-hidden="true" tabindex="-1"></a>        upsample5 <span class="op">=</span> <span class="va">self</span>._block3(pool5)</span>
<span id="cb31-83"><a href="#cb31-83" aria-hidden="true" tabindex="-1"></a>        concat5 <span class="op">=</span> torch.cat((upsample5, pool4), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-84"><a href="#cb31-84" aria-hidden="true" tabindex="-1"></a>        upsample4 <span class="op">=</span> <span class="va">self</span>._block4(concat5)</span>
<span id="cb31-85"><a href="#cb31-85" aria-hidden="true" tabindex="-1"></a>        concat4 <span class="op">=</span> torch.cat((upsample4, pool3), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-86"><a href="#cb31-86" aria-hidden="true" tabindex="-1"></a>        upsample3 <span class="op">=</span> <span class="va">self</span>._block5(concat4)</span>
<span id="cb31-87"><a href="#cb31-87" aria-hidden="true" tabindex="-1"></a>        concat3 <span class="op">=</span> torch.cat((upsample3, pool2), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-88"><a href="#cb31-88" aria-hidden="true" tabindex="-1"></a>        upsample2 <span class="op">=</span> <span class="va">self</span>._block5(concat3)</span>
<span id="cb31-89"><a href="#cb31-89" aria-hidden="true" tabindex="-1"></a>        concat2 <span class="op">=</span> torch.cat((upsample2, pool1), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-90"><a href="#cb31-90" aria-hidden="true" tabindex="-1"></a>        upsample1 <span class="op">=</span> <span class="va">self</span>._block5(concat2)</span>
<span id="cb31-91"><a href="#cb31-91" aria-hidden="true" tabindex="-1"></a>        concat1 <span class="op">=</span> torch.cat((upsample1, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-92"><a href="#cb31-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-93"><a href="#cb31-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final activation</span></span>
<span id="cb31-94"><a href="#cb31-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._block6(concat1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now I will create noisy inputs and targets from the .tif stacks I have. I will be using 4 images with 10 noisy instances each for testing. This is a realistic capture budget you might see in the field. I could do more, but I have limited access to gpus, and I want to see if this is realistic for someone to use in the field. .</p>
<div id="fa103c8e" class="cell" data-outputid="508239d8-829b-4426-8eb0-26188a330f7b">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">'/content/gdrive/MyDrive/CS445Final/2023-08-30-FullDataSet'</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> []</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> []</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, image_class <span class="kw">in</span> <span class="bu">enumerate</span>(os.listdir(path)):</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    sameBaseImages <span class="op">=</span> []</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    image_path <span class="op">=</span> os.path.join(path, image_class)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    tif <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co">#only 10 instances per image.</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        tif.seek(i)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> np.array(tif)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        sameBaseImages.append(img)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#randomly match up different noise instances of the same image</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    shuffled_images <span class="op">=</span> sameBaseImages.copy()</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    random.shuffle(shuffled_images)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    X.append(shuffled_images)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    y.append(sameBaseImages)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ind <span class="op">==</span> <span class="dv">3</span>: <span class="co">#create capture budget of only 4 images.</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([item <span class="cf">for</span> row <span class="kw">in</span> X <span class="cf">for</span> item <span class="kw">in</span> row]) <span class="co">#flatten</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([item <span class="cf">for</span> row <span class="kw">in</span> y <span class="cf">for</span> item <span class="kw">in</span> row])</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>plt.imshow(X[<span class="dv">0</span>])</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Input"</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>plt.imshow(y[<span class="dv">0</span>])</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Target"</span>)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.from_numpy(X)</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.from_numpy(y)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X and y shape: "</span> ,X.shape , y.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X and y shape:  torch.Size([40, 1, 128, 128]) torch.Size([40, 1, 128, 128])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now initialize and train. We will be using L2 loss (though you can chose other loss metrics), and Adam. An interesing aspect of Noise2Noise is that there is often no validation or test set because the training set output is actually what we want. The traning image outputs are denoised!</p>
<div id="ff60f0ba" class="cell" data-outputid="84694106-f973-4272-858b-9bbd14a56970">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> UNet()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> Adam(model.parameters())</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>use_cuda <span class="op">=</span> <span class="va">False</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    use_cuda <span class="op">=</span> <span class="va">True</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.cuda()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.cuda()</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y.cuda()</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>nb_epochs <span class="op">=</span> <span class="dv">250</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>denoisedExamples <span class="op">=</span> []</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>lossTrace <span class="op">=</span> []</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(nb_epochs):</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    denoised <span class="op">=</span> model(X)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    denoisedExamples.append(denoised)</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(denoised, y)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    optim.zero_grad()</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    optim.step()</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>    lossTrace.append(loss.item())</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">'EPOCH </span><span class="sc">{:d}</span><span class="st"> / </span><span class="sc">{:d}</span><span class="st">'</span>.<span class="bu">format</span>(epoch <span class="op">+</span> <span class="dv">1</span>, nb_epochs) , <span class="st">"Loss: "</span> , loss.item())</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training done."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>EPOCH 1 / 250 Loss:  0.29845336079597473
EPOCH 11 / 250 Loss:  0.054240882396698
EPOCH 21 / 250 Loss:  0.0514160580933094
EPOCH 31 / 250 Loss:  0.048928435891866684
EPOCH 41 / 250 Loss:  0.047724079340696335
EPOCH 51 / 250 Loss:  0.0469043031334877
EPOCH 61 / 250 Loss:  0.046384233981370926
EPOCH 71 / 250 Loss:  0.04599473997950554
EPOCH 81 / 250 Loss:  0.045712895691394806
EPOCH 91 / 250 Loss:  0.04553460702300072
EPOCH 101 / 250 Loss:  0.04540208354592323
EPOCH 111 / 250 Loss:  0.04528817534446716
EPOCH 121 / 250 Loss:  0.04519662261009216
EPOCH 131 / 250 Loss:  0.045131005346775055
EPOCH 141 / 250 Loss:  0.045083049684762955
EPOCH 151 / 250 Loss:  0.045046743005514145
EPOCH 161 / 250 Loss:  0.045003242790699005
EPOCH 171 / 250 Loss:  0.04496356472373009
EPOCH 181 / 250 Loss:  0.04492597654461861
EPOCH 191 / 250 Loss:  0.044932108372449875
EPOCH 201 / 250 Loss:  0.044871117919683456
EPOCH 211 / 250 Loss:  0.04482579603791237
EPOCH 221 / 250 Loss:  0.04478657618165016
EPOCH 231 / 250 Loss:  0.04479914531111717
EPOCH 241 / 250 Loss:  0.04471750184893608
Training done.</code></pre>
</div>
</div>
<div id="c9b82ff4" class="cell" data-outputid="512aaed3-126b-4b13-8244-b0f38520fa0b">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(np.arange(nb_epochs), lossTrace)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Loss Trace'</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'MSE'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.ylim(0, .1)</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="4ace623f" class="cell" data-outputid="6f06e9ef-0bd1-4c4e-8e9f-20ad932aa3cc">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> (<span class="dv">2</span>,<span class="dv">5</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nb_epochs):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">25</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        plt.subplot(rows, cols, i<span class="op">//</span> <span class="dv">25</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        plt.imshow(denoisedExamples[i].cpu().detach().numpy()[<span class="dv">0</span>].reshape(<span class="dv">128</span>,<span class="dv">128</span>))</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f'Epoch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="-lQWommMivSO" class="cell" data-outputid="c8e648df-074c-4e01-bba3-8169151ba9b2" data-execution_count="86">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(a[<span class="dv">0</span>])</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Average of Stack'</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Noise2Noise does amazing. There is clearly underlying stucture that the model is finding. Unless the model completely memorizes the training set and ends up predicting the noisy target exactly, mathematically it has to be revealing true structure.</p>
<p>With a capture budget of 10 noisy instances for 4 images (40 total images), we can get very good denoising results. We do not need to know the noise distribution, the model learns it for us.</p>
<p>An issue with this method is inference on new images. That is what some of my research has to deal with, and is not something that I want to get into in this project.</p>
<section id="pros-2" class="level4">
<h4 class="anchored" data-anchor-id="pros-2">Pros:</h4>
<ul>
<li>Can use information from multiple images</li>
<li>Excellent denoising.</li>
<li>Versatile on many types of noise.</li>
<li>Does not create artifacts or new structure (in most circumstances).</li>
<li>Very few hyperparameters</li>
</ul>
</section>
<section id="cons-2" class="level4">
<h4 class="anchored" data-anchor-id="cons-2">Cons:</h4>
<ul>
<li>Complex.</li>
<li>Takes a good amount of time and is computationaly expensive.</li>
<li>Using this for inference is difficult.</li>
<li><em>Requires</em> noisy image pairs (this is often impossible).</li>
</ul>
<p>Noise2Noise provides confident predictions, is easy to use and understand (once you think about if for a while), and is hard to mess up. I will comment more on Noise2Noise is the conclusion.</p>
<hr>
<hr>
</section>
</section>
<section id="noise2void" class="level3">
<h3 class="anchored" data-anchor-id="noise2void"><strong>Noise2Void</strong></h3>
<p>[Krull et al., 2019]</p>
<p>As I mentioned, Noise2Noise has been built upon alot. Noise2Void is one of the most popular contributions to Noise2Noise, and I want to explore it.</p>
<p>The basic premise of Noise2Void is that the ground truth has underlying structure, and noise does not. Therefore we can predict the ground truth using the surrounding pixels and we cannot predict noise from the surrounding pixels. This method takes advantage of patch creation, like many other denoisers. Noise2Void ‘blinds’ the middle pixel of a patch of pixels, and uses the rest of the surrounding pixels to predict that blinded pixel.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-Figure2-1.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>[Krull et al., 2019]</p>
<p>The framework only requires one image (not pairs) for denoising because it creates patches out of the images and those become the inputs and targets.</p>
<p>I will be using a tensor flow implementation for this because it was the only way I could find how to do it. I followed this tutorial https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb and still struggled because of all of the moving parts. The training for this framework is a bit more involved, with the creation of patches and blinding pixels.</p>
<div id="801d3eac" class="cell" data-outputid="a0b3e99b-c47e-444c-9919-49b097ca23a2" data-execution_count="185">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Noise2Void tutorial taken from https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>datagen <span class="op">=</span> N2V_DataGenerator()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> datagen.load_imgs_from_directory(<span class="st">'/content/gdrive/MyDrive/CS445Final/Png2'</span>,<span class="bu">filter</span><span class="op">=</span><span class="st">'*.png'</span>,dims<span class="op">=</span><span class="st">'XY'</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of loaded images: '</span>, imgs[<span class="dv">0</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>shape of loaded images:  (1, 128, 128, 1)</code></pre>
</div>
</div>
<div id="wbcFvS9tPa89" class="cell" data-outputid="8ea7b788-9a65-42a8-eba7-9ba422903bf1" data-execution_count="186">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>patch_shape <span class="op">=</span> (patch_size,patch_size)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>patches <span class="op">=</span> datagen.generate_patches_from_list(imgs, shape<span class="op">=</span>patch_shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)
Generated patches: (128, 32, 32, 1)</code></pre>
</div>
</div>
<div id="tcQsBn9EPbB2" class="cell" data-outputid="e7ad2ead-b7d3-46db-ae7b-5a8ca5d74b20" data-execution_count="181">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">max</span>(patches[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="181">
<pre><code>0.91764706</code></pre>
</div>
</div>
<div id="mZdzH0rkR0c2" class="cell" data-outputid="0c5f8bf7-40b2-49c7-8a21-83ad97b68762" data-execution_count="187">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>train_val_split <span class="op">=</span> <span class="bu">int</span>(patches.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>X_patch <span class="op">=</span> patches[:train_val_split]</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> patches[train_val_split:]</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_patch.shape)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_val.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1843, 32, 32, 1)
(461, 32, 32, 1)</code></pre>
</div>
</div>
<div id="Q-l0qZe1R-A9" class="cell" data-execution_count="188">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>train_batch <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> N2VConfig(X_patch, unet_kern_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>                   unet_n_first<span class="op">=</span><span class="dv">64</span>, unet_n_depth<span class="op">=</span><span class="dv">3</span>, train_steps_per_epoch<span class="op">=</span><span class="bu">int</span>(X_patch.shape[<span class="dv">0</span>]<span class="op">/</span>train_batch), train_epochs<span class="op">=</span><span class="dv">100</span>, train_loss<span class="op">=</span><span class="st">'mse'</span>,</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>                   batch_norm<span class="op">=</span><span class="va">True</span>, train_batch_size<span class="op">=</span>train_batch, n2v_perc_pix<span class="op">=</span><span class="fl">0.198</span>, n2v_patch_shape<span class="op">=</span>(patch_size, patch_size),</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>                   n2v_manipulator<span class="op">=</span><span class="st">'uniform_withCP'</span>, n2v_neighborhood_radius<span class="op">=</span><span class="dv">5</span>, single_net_per_channel<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="OfskWrqGSITC" class="cell" data-outputid="29f817c9-9f54-4c1c-d441-6d68cd9f3e66" data-execution_count="189">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">'n2vModel32'</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>basedir <span class="op">=</span> <span class="st">'/content/gdrive/MyDrive/CS445Final'</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> N2V(config, model_name, basedir<span class="op">=</span>basedir)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.train(X_patch, X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/n2v/models/n2v_standard.py:447: UserWarning: output path for model already exists, files may be overwritten: /content/gdrive/MyDrive/CS445Final/n2vModel32
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2 blind-spots will be generated per training patch of size (32, 32).</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Preparing validation data: 100%|██████████| 461/461 [00:00&lt;00:00, 7139.75it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 5/57 [=&gt;............................] - ETA: 1s - loss: 2.0214 - n2v_mse: 2.0214 - n2v_abs: 1.1068</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0163s). Check your callbacks.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 10s 50ms/step - loss: 1.2380 - n2v_mse: 1.2380 - n2v_abs: 0.8616 - val_loss: 1.0978 - val_n2v_mse: 1.1045 - val_n2v_abs: 0.8492 - lr: 4.0000e-04
Epoch 2/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 42ms/step - loss: 0.8504 - n2v_mse: 0.8504 - n2v_abs: 0.7049 - val_loss: 0.8798 - val_n2v_mse: 0.8823 - val_n2v_abs: 0.7305 - lr: 4.0000e-04
Epoch 3/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 34ms/step - loss: 0.7648 - n2v_mse: 0.7648 - n2v_abs: 0.6723 - val_loss: 0.8481 - val_n2v_mse: 0.8522 - val_n2v_abs: 0.7116 - lr: 4.0000e-04
Epoch 4/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7562 - n2v_mse: 0.7562 - n2v_abs: 0.6755 - val_loss: 0.8817 - val_n2v_mse: 0.8845 - val_n2v_abs: 0.7294 - lr: 4.0000e-04
Epoch 5/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.7011 - n2v_mse: 0.7011 - n2v_abs: 0.6465 - val_loss: 0.7806 - val_n2v_mse: 0.7772 - val_n2v_abs: 0.6585 - lr: 4.0000e-04
Epoch 6/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7267 - n2v_mse: 0.7267 - n2v_abs: 0.6553 - val_loss: 0.7924 - val_n2v_mse: 0.7890 - val_n2v_abs: 0.6711 - lr: 4.0000e-04
Epoch 7/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7334 - n2v_mse: 0.7334 - n2v_abs: 0.6585 - val_loss: 0.8120 - val_n2v_mse: 0.8111 - val_n2v_abs: 0.6791 - lr: 4.0000e-04
Epoch 8/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 35ms/step - loss: 0.7403 - n2v_mse: 0.7403 - n2v_abs: 0.6673 - val_loss: 0.7853 - val_n2v_mse: 0.7832 - val_n2v_abs: 0.6545 - lr: 4.0000e-04
Epoch 9/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 42ms/step - loss: 0.6958 - n2v_mse: 0.6958 - n2v_abs: 0.6519 - val_loss: 0.7972 - val_n2v_mse: 0.7955 - val_n2v_abs: 0.6703 - lr: 4.0000e-04
Epoch 10/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.7023 - n2v_mse: 0.7023 - n2v_abs: 0.6464 - val_loss: 0.7861 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6659 - lr: 4.0000e-04
Epoch 11/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7464 - n2v_mse: 0.7464 - n2v_abs: 0.6638 - val_loss: 0.8975 - val_n2v_mse: 0.9165 - val_n2v_abs: 0.7041 - lr: 4.0000e-04
Epoch 12/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7545 - n2v_mse: 0.7545 - n2v_abs: 0.6724 - val_loss: 0.8093 - val_n2v_mse: 0.8029 - val_n2v_abs: 0.6655 - lr: 4.0000e-04
Epoch 13/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7051 - n2v_mse: 0.7051 - n2v_abs: 0.6418 - val_loss: 0.9975 - val_n2v_mse: 1.0156 - val_n2v_abs: 0.7214 - lr: 4.0000e-04
Epoch 14/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7441 - n2v_mse: 0.7441 - n2v_abs: 0.6639 - val_loss: 0.8361 - val_n2v_mse: 0.8282 - val_n2v_abs: 0.6740 - lr: 4.0000e-04
Epoch 15/100
57/57 [==============================] - ETA: 0s - loss: 0.7297 - n2v_mse: 0.7297 - n2v_abs: 0.6520
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00019999999494757503.
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 35ms/step - loss: 0.7297 - n2v_mse: 0.7297 - n2v_abs: 0.6520 - val_loss: 0.8968 - val_n2v_mse: 0.8907 - val_n2v_abs: 0.6887 - lr: 4.0000e-04
Epoch 16/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.6787 - n2v_mse: 0.6787 - n2v_abs: 0.6377 - val_loss: 0.7861 - val_n2v_mse: 0.7850 - val_n2v_abs: 0.6583 - lr: 2.0000e-04
Epoch 17/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7304 - n2v_mse: 0.7304 - n2v_abs: 0.6591 - val_loss: 0.7834 - val_n2v_mse: 0.7815 - val_n2v_abs: 0.6513 - lr: 2.0000e-04
Epoch 18/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6944 - n2v_mse: 0.6944 - n2v_abs: 0.6444 - val_loss: 0.7926 - val_n2v_mse: 0.7897 - val_n2v_abs: 0.6616 - lr: 2.0000e-04
Epoch 19/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 44ms/step - loss: 0.7156 - n2v_mse: 0.7156 - n2v_abs: 0.6450 - val_loss: 0.7760 - val_n2v_mse: 0.7750 - val_n2v_abs: 0.6539 - lr: 2.0000e-04
Epoch 20/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6908 - n2v_mse: 0.6908 - n2v_abs: 0.6367 - val_loss: 0.7680 - val_n2v_mse: 0.7683 - val_n2v_abs: 0.6480 - lr: 2.0000e-04
Epoch 21/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6974 - n2v_mse: 0.6974 - n2v_abs: 0.6381 - val_loss: 0.7815 - val_n2v_mse: 0.7801 - val_n2v_abs: 0.6540 - lr: 2.0000e-04
Epoch 22/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 38ms/step - loss: 0.7034 - n2v_mse: 0.7034 - n2v_abs: 0.6403 - val_loss: 0.7691 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6511 - lr: 2.0000e-04
Epoch 23/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 36ms/step - loss: 0.6924 - n2v_mse: 0.6924 - n2v_abs: 0.6440 - val_loss: 0.7710 - val_n2v_mse: 0.7707 - val_n2v_abs: 0.6520 - lr: 2.0000e-04
Epoch 24/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7199 - n2v_mse: 0.7199 - n2v_abs: 0.6554 - val_loss: 0.7912 - val_n2v_mse: 0.7878 - val_n2v_abs: 0.6523 - lr: 2.0000e-04
Epoch 25/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6844 - n2v_mse: 0.6844 - n2v_abs: 0.6372 - val_loss: 0.7967 - val_n2v_mse: 0.8029 - val_n2v_abs: 0.6640 - lr: 2.0000e-04
Epoch 26/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.7252 - n2v_mse: 0.7252 - n2v_abs: 0.6519 - val_loss: 0.7870 - val_n2v_mse: 0.7870 - val_n2v_abs: 0.6566 - lr: 2.0000e-04
Epoch 27/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.7058 - n2v_mse: 0.7058 - n2v_abs: 0.6417 - val_loss: 0.7950 - val_n2v_mse: 0.7926 - val_n2v_abs: 0.6671 - lr: 2.0000e-04
Epoch 28/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7087 - n2v_mse: 0.7087 - n2v_abs: 0.6473 - val_loss: 0.8128 - val_n2v_mse: 0.8106 - val_n2v_abs: 0.6593 - lr: 2.0000e-04
Epoch 29/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 35ms/step - loss: 0.7035 - n2v_mse: 0.7035 - n2v_abs: 0.6409 - val_loss: 0.7808 - val_n2v_mse: 0.7793 - val_n2v_abs: 0.6586 - lr: 2.0000e-04
Epoch 30/100
57/57 [==============================] - ETA: 0s - loss: 0.7311 - n2v_mse: 0.7311 - n2v_abs: 0.6555
Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.7311 - n2v_mse: 0.7311 - n2v_abs: 0.6555 - val_loss: 0.9077 - val_n2v_mse: 0.9252 - val_n2v_abs: 0.7188 - lr: 2.0000e-04
Epoch 31/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7306 - n2v_mse: 0.7306 - n2v_abs: 0.6554 - val_loss: 0.7851 - val_n2v_mse: 0.7793 - val_n2v_abs: 0.6535 - lr: 1.0000e-04
Epoch 32/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.6730 - n2v_mse: 0.6730 - n2v_abs: 0.6306 - val_loss: 0.7857 - val_n2v_mse: 0.7812 - val_n2v_abs: 0.6494 - lr: 1.0000e-04
Epoch 33/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.6890 - n2v_mse: 0.6890 - n2v_abs: 0.6480 - val_loss: 0.7715 - val_n2v_mse: 0.7763 - val_n2v_abs: 0.6528 - lr: 1.0000e-04
Epoch 34/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6914 - n2v_mse: 0.6914 - n2v_abs: 0.6420 - val_loss: 0.7749 - val_n2v_mse: 0.7758 - val_n2v_abs: 0.6536 - lr: 1.0000e-04
Epoch 35/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7093 - n2v_mse: 0.7093 - n2v_abs: 0.6408 - val_loss: 0.7970 - val_n2v_mse: 0.7926 - val_n2v_abs: 0.6643 - lr: 1.0000e-04
Epoch 36/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.7217 - n2v_mse: 0.7217 - n2v_abs: 0.6568 - val_loss: 0.7807 - val_n2v_mse: 0.7782 - val_n2v_abs: 0.6536 - lr: 1.0000e-04
Epoch 37/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 41ms/step - loss: 0.6887 - n2v_mse: 0.6887 - n2v_abs: 0.6415 - val_loss: 0.7845 - val_n2v_mse: 0.7857 - val_n2v_abs: 0.6603 - lr: 1.0000e-04
Epoch 38/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7151 - n2v_mse: 0.7151 - n2v_abs: 0.6535 - val_loss: 0.7707 - val_n2v_mse: 0.7719 - val_n2v_abs: 0.6523 - lr: 1.0000e-04
Epoch 39/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.6591 - n2v_mse: 0.6591 - n2v_abs: 0.6271 - val_loss: 0.7750 - val_n2v_mse: 0.7786 - val_n2v_abs: 0.6555 - lr: 1.0000e-04
Epoch 40/100
56/57 [============================&gt;.] - ETA: 0s - loss: 0.6680 - n2v_mse: 0.6680 - n2v_abs: 0.6305
Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
3/3 [==============================] - 0s 4ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.6739 - n2v_mse: 0.6739 - n2v_abs: 0.6316 - val_loss: 0.7722 - val_n2v_mse: 0.7714 - val_n2v_abs: 0.6524 - lr: 1.0000e-04
Epoch 41/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 3s 44ms/step - loss: 0.6916 - n2v_mse: 0.6916 - n2v_abs: 0.6379 - val_loss: 0.7656 - val_n2v_mse: 0.7646 - val_n2v_abs: 0.6490 - lr: 5.0000e-05
Epoch 42/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6847 - n2v_mse: 0.6847 - n2v_abs: 0.6387 - val_loss: 0.7670 - val_n2v_mse: 0.7658 - val_n2v_abs: 0.6486 - lr: 5.0000e-05
Epoch 43/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.7018 - n2v_mse: 0.7018 - n2v_abs: 0.6402 - val_loss: 0.7839 - val_n2v_mse: 0.7962 - val_n2v_abs: 0.6567 - lr: 5.0000e-05
Epoch 44/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 38ms/step - loss: 0.6524 - n2v_mse: 0.6524 - n2v_abs: 0.6241 - val_loss: 0.7722 - val_n2v_mse: 0.7769 - val_n2v_abs: 0.6497 - lr: 5.0000e-05
Epoch 45/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.7115 - n2v_mse: 0.7115 - n2v_abs: 0.6516 - val_loss: 0.7937 - val_n2v_mse: 0.8046 - val_n2v_abs: 0.6589 - lr: 5.0000e-05
Epoch 46/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6811 - n2v_mse: 0.6811 - n2v_abs: 0.6325 - val_loss: 0.7839 - val_n2v_mse: 0.7812 - val_n2v_abs: 0.6509 - lr: 5.0000e-05
Epoch 47/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6985 - n2v_mse: 0.6985 - n2v_abs: 0.6472 - val_loss: 0.7842 - val_n2v_mse: 0.7842 - val_n2v_abs: 0.6569 - lr: 5.0000e-05
Epoch 48/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6868 - n2v_mse: 0.6868 - n2v_abs: 0.6385 - val_loss: 0.7881 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6524 - lr: 5.0000e-05
Epoch 49/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7152 - n2v_mse: 0.7152 - n2v_abs: 0.6438 - val_loss: 0.7766 - val_n2v_mse: 0.7790 - val_n2v_abs: 0.6523 - lr: 5.0000e-05
Epoch 50/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 35ms/step - loss: 0.6980 - n2v_mse: 0.6980 - n2v_abs: 0.6433 - val_loss: 0.7755 - val_n2v_mse: 0.7768 - val_n2v_abs: 0.6513 - lr: 5.0000e-05
Epoch 51/100
56/57 [============================&gt;.] - ETA: 0s - loss: 0.7021 - n2v_mse: 0.7021 - n2v_abs: 0.6382
Epoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 42ms/step - loss: 0.6995 - n2v_mse: 0.6995 - n2v_abs: 0.6380 - val_loss: 0.7763 - val_n2v_mse: 0.7752 - val_n2v_abs: 0.6532 - lr: 5.0000e-05
Epoch 52/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7246 - n2v_mse: 0.7246 - n2v_abs: 0.6502 - val_loss: 0.7772 - val_n2v_mse: 0.7757 - val_n2v_abs: 0.6527 - lr: 2.5000e-05
Epoch 53/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6618 - n2v_mse: 0.6618 - n2v_abs: 0.6341 - val_loss: 0.7818 - val_n2v_mse: 0.7823 - val_n2v_abs: 0.6554 - lr: 2.5000e-05
Epoch 54/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6962 - n2v_mse: 0.6962 - n2v_abs: 0.6420 - val_loss: 0.7719 - val_n2v_mse: 0.7729 - val_n2v_abs: 0.6510 - lr: 2.5000e-05
Epoch 55/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 3s 47ms/step - loss: 0.7091 - n2v_mse: 0.7091 - n2v_abs: 0.6425 - val_loss: 0.7654 - val_n2v_mse: 0.7657 - val_n2v_abs: 0.6479 - lr: 2.5000e-05
Epoch 56/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6999 - n2v_mse: 0.6999 - n2v_abs: 0.6449 - val_loss: 0.7723 - val_n2v_mse: 0.7689 - val_n2v_abs: 0.6497 - lr: 2.5000e-05
Epoch 57/100
3/3 [==============================] - 0s 10ms/step
57/57 [==============================] - 2s 40ms/step - loss: 0.7027 - n2v_mse: 0.7027 - n2v_abs: 0.6471 - val_loss: 0.7787 - val_n2v_mse: 0.7739 - val_n2v_abs: 0.6501 - lr: 2.5000e-05
Epoch 58/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 38ms/step - loss: 0.7022 - n2v_mse: 0.7022 - n2v_abs: 0.6393 - val_loss: 0.7762 - val_n2v_mse: 0.7718 - val_n2v_abs: 0.6500 - lr: 2.5000e-05
Epoch 59/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6751 - n2v_mse: 0.6751 - n2v_abs: 0.6327 - val_loss: 0.7768 - val_n2v_mse: 0.7734 - val_n2v_abs: 0.6508 - lr: 2.5000e-05
Epoch 60/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7271 - n2v_mse: 0.7271 - n2v_abs: 0.6578 - val_loss: 0.7822 - val_n2v_mse: 0.7769 - val_n2v_abs: 0.6517 - lr: 2.5000e-05
Epoch 61/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6944 - n2v_mse: 0.6944 - n2v_abs: 0.6355 - val_loss: 0.7807 - val_n2v_mse: 0.7756 - val_n2v_abs: 0.6520 - lr: 2.5000e-05
Epoch 62/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7125 - n2v_mse: 0.7125 - n2v_abs: 0.6567 - val_loss: 0.7727 - val_n2v_mse: 0.7697 - val_n2v_abs: 0.6497 - lr: 2.5000e-05
Epoch 63/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7025 - n2v_mse: 0.7025 - n2v_abs: 0.6433 - val_loss: 0.7732 - val_n2v_mse: 0.7704 - val_n2v_abs: 0.6506 - lr: 2.5000e-05
Epoch 64/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.7020 - n2v_mse: 0.7020 - n2v_abs: 0.6453 - val_loss: 0.7739 - val_n2v_mse: 0.7746 - val_n2v_abs: 0.6537 - lr: 2.5000e-05
Epoch 65/100
57/57 [==============================] - ETA: 0s - loss: 0.7049 - n2v_mse: 0.7049 - n2v_abs: 0.6479
Epoch 65: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 40ms/step - loss: 0.7049 - n2v_mse: 0.7049 - n2v_abs: 0.6479 - val_loss: 0.7814 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6586 - lr: 2.5000e-05
Epoch 66/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6902 - n2v_mse: 0.6902 - n2v_abs: 0.6410 - val_loss: 0.7746 - val_n2v_mse: 0.7735 - val_n2v_abs: 0.6529 - lr: 1.2500e-05
Epoch 67/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6791 - n2v_mse: 0.6791 - n2v_abs: 0.6347 - val_loss: 0.7734 - val_n2v_mse: 0.7727 - val_n2v_abs: 0.6518 - lr: 1.2500e-05
Epoch 68/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6996 - n2v_mse: 0.6996 - n2v_abs: 0.6412 - val_loss: 0.7699 - val_n2v_mse: 0.7683 - val_n2v_abs: 0.6485 - lr: 1.2500e-05
Epoch 69/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6849 - n2v_mse: 0.6849 - n2v_abs: 0.6431 - val_loss: 0.7678 - val_n2v_mse: 0.7662 - val_n2v_abs: 0.6474 - lr: 1.2500e-05
Epoch 70/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6889 - n2v_mse: 0.6889 - n2v_abs: 0.6347 - val_loss: 0.7678 - val_n2v_mse: 0.7664 - val_n2v_abs: 0.6478 - lr: 1.2500e-05
Epoch 71/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.6556 - n2v_mse: 0.6556 - n2v_abs: 0.6218 - val_loss: 0.7693 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6495 - lr: 1.2500e-05
Epoch 72/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 40ms/step - loss: 0.6963 - n2v_mse: 0.6963 - n2v_abs: 0.6397 - val_loss: 0.7698 - val_n2v_mse: 0.7695 - val_n2v_abs: 0.6504 - lr: 1.2500e-05
Epoch 73/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6787 - n2v_mse: 0.6787 - n2v_abs: 0.6371 - val_loss: 0.7670 - val_n2v_mse: 0.7663 - val_n2v_abs: 0.6486 - lr: 1.2500e-05
Epoch 74/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6911 - n2v_mse: 0.6911 - n2v_abs: 0.6462 - val_loss: 0.7691 - val_n2v_mse: 0.7687 - val_n2v_abs: 0.6501 - lr: 1.2500e-05
Epoch 75/100
57/57 [==============================] - ETA: 0s - loss: 0.6884 - n2v_mse: 0.6884 - n2v_abs: 0.6378
Epoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6884 - n2v_mse: 0.6884 - n2v_abs: 0.6378 - val_loss: 0.7698 - val_n2v_mse: 0.7688 - val_n2v_abs: 0.6504 - lr: 1.2500e-05
Epoch 76/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6933 - n2v_mse: 0.6933 - n2v_abs: 0.6390 - val_loss: 0.7708 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6500 - lr: 6.2500e-06
Epoch 77/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6919 - n2v_mse: 0.6919 - n2v_abs: 0.6348 - val_loss: 0.7704 - val_n2v_mse: 0.7680 - val_n2v_abs: 0.6493 - lr: 6.2500e-06
Epoch 78/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 40ms/step - loss: 0.6793 - n2v_mse: 0.6793 - n2v_abs: 0.6372 - val_loss: 0.7706 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6485 - lr: 6.2500e-06
Epoch 79/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.6649 - n2v_mse: 0.6649 - n2v_abs: 0.6262 - val_loss: 0.7696 - val_n2v_mse: 0.7668 - val_n2v_abs: 0.6481 - lr: 6.2500e-06
Epoch 80/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.7041 - n2v_mse: 0.7041 - n2v_abs: 0.6483 - val_loss: 0.7701 - val_n2v_mse: 0.7678 - val_n2v_abs: 0.6487 - lr: 6.2500e-06
Epoch 81/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 30ms/step - loss: 0.7154 - n2v_mse: 0.7154 - n2v_abs: 0.6533 - val_loss: 0.7690 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6490 - lr: 6.2500e-06
Epoch 82/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7028 - n2v_mse: 0.7028 - n2v_abs: 0.6440 - val_loss: 0.7686 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6486 - lr: 6.2500e-06
Epoch 83/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6957 - n2v_mse: 0.6957 - n2v_abs: 0.6417 - val_loss: 0.7701 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6498 - lr: 6.2500e-06
Epoch 84/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6887 - n2v_mse: 0.6887 - n2v_abs: 0.6347 - val_loss: 0.7708 - val_n2v_mse: 0.7690 - val_n2v_abs: 0.6499 - lr: 6.2500e-06
Epoch 85/100
56/57 [============================&gt;.] - ETA: 0s - loss: 0.6920 - n2v_mse: 0.6920 - n2v_abs: 0.6394
Epoch 85: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
3/3 [==============================] - 0s 8ms/step
57/57 [==============================] - 2s 37ms/step - loss: 0.6924 - n2v_mse: 0.6924 - n2v_abs: 0.6385 - val_loss: 0.7695 - val_n2v_mse: 0.7677 - val_n2v_abs: 0.6488 - lr: 6.2500e-06
Epoch 86/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.6830 - n2v_mse: 0.6830 - n2v_abs: 0.6371 - val_loss: 0.7685 - val_n2v_mse: 0.7670 - val_n2v_abs: 0.6481 - lr: 3.1250e-06
Epoch 87/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6937 - n2v_mse: 0.6937 - n2v_abs: 0.6415 - val_loss: 0.7685 - val_n2v_mse: 0.7670 - val_n2v_abs: 0.6481 - lr: 3.1250e-06
Epoch 88/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6890 - n2v_mse: 0.6890 - n2v_abs: 0.6430 - val_loss: 0.7684 - val_n2v_mse: 0.7665 - val_n2v_abs: 0.6477 - lr: 3.1250e-06
Epoch 89/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6909 - n2v_mse: 0.6909 - n2v_abs: 0.6375 - val_loss: 0.7692 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6483 - lr: 3.1250e-06
Epoch 90/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 33ms/step - loss: 0.6661 - n2v_mse: 0.6661 - n2v_abs: 0.6334 - val_loss: 0.7696 - val_n2v_mse: 0.7676 - val_n2v_abs: 0.6487 - lr: 3.1250e-06
Epoch 91/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.7051 - n2v_mse: 0.7051 - n2v_abs: 0.6480 - val_loss: 0.7694 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6487 - lr: 3.1250e-06
Epoch 92/100
3/3 [==============================] - 0s 9ms/step
57/57 [==============================] - 2s 37ms/step - loss: 0.6599 - n2v_mse: 0.6599 - n2v_abs: 0.6272 - val_loss: 0.7692 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6486 - lr: 3.1250e-06
Epoch 93/100
3/3 [==============================] - 0s 8ms/step
57/57 [==============================] - 2s 39ms/step - loss: 0.7332 - n2v_mse: 0.7332 - n2v_abs: 0.6577 - val_loss: 0.7687 - val_n2v_mse: 0.7667 - val_n2v_abs: 0.6481 - lr: 3.1250e-06
Epoch 94/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6783 - n2v_mse: 0.6783 - n2v_abs: 0.6348 - val_loss: 0.7691 - val_n2v_mse: 0.7675 - val_n2v_abs: 0.6485 - lr: 3.1250e-06
Epoch 95/100
55/57 [===========================&gt;..] - ETA: 0s - loss: 0.6860 - n2v_mse: 0.6860 - n2v_abs: 0.6357
Epoch 95: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6868 - n2v_mse: 0.6868 - n2v_abs: 0.6359 - val_loss: 0.7698 - val_n2v_mse: 0.7685 - val_n2v_abs: 0.6491 - lr: 3.1250e-06
Epoch 96/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6953 - n2v_mse: 0.6953 - n2v_abs: 0.6423 - val_loss: 0.7693 - val_n2v_mse: 0.7680 - val_n2v_abs: 0.6490 - lr: 1.5625e-06
Epoch 97/100
3/3 [==============================] - 0s 6ms/step
57/57 [==============================] - 2s 31ms/step - loss: 0.6853 - n2v_mse: 0.6853 - n2v_abs: 0.6327 - val_loss: 0.7690 - val_n2v_mse: 0.7677 - val_n2v_abs: 0.6487 - lr: 1.5625e-06
Epoch 98/100
3/3 [==============================] - 0s 5ms/step
57/57 [==============================] - 2s 32ms/step - loss: 0.6939 - n2v_mse: 0.6939 - n2v_abs: 0.6413 - val_loss: 0.7683 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6484 - lr: 1.5625e-06
Epoch 99/100
3/3 [==============================] - 0s 9ms/step
57/57 [==============================] - 2s 35ms/step - loss: 0.6810 - n2v_mse: 0.6810 - n2v_abs: 0.6378 - val_loss: 0.7684 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6485 - lr: 1.5625e-06
Epoch 100/100
3/3 [==============================] - 0s 7ms/step
57/57 [==============================] - 2s 40ms/step - loss: 0.6949 - n2v_mse: 0.6949 - n2v_abs: 0.6387 - val_loss: 0.7685 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6485 - lr: 1.5625e-06

Loading network weights from 'weights_best.h5'.</code></pre>
</div>
</div>
<div id="ZgTgdB-pSCKk" class="cell" data-outputid="78ec857d-04db-40e2-9bcd-d0a78b28733f" data-execution_count="190">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">5</span>))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>plot_history(history,[<span class="st">'loss'</span>,<span class="st">'val_loss'</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="yhZWw-6nYqeV" class="cell" data-outputid="5506e990-26d3-4f50-d77d-f29b8093f90a" data-execution_count="191">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">'n2vModel32'</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>basedir <span class="op">=</span> <span class="st">'/content/gdrive/MyDrive/CS445Final'</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> N2V(config<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span>model_name, basedir<span class="op">=</span>basedir)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.image <span class="im">import</span> imread</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> imread(<span class="st">'/content/gdrive/MyDrive/CS445Final/Png2/20230830_15_40_14.png'</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(img, axes<span class="op">=</span><span class="st">'YX'</span>)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="dv">0</span>], pred, a[<span class="dv">0</span>], <span class="st">'Noise2Void Output'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading network weights from 'weights_best.h5'.
1/1 [==============================] - 0s 299ms/step
MSE (denoised vs average):  1.03049</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-28-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="deIo6HgHKHvB" class="cell" data-outputid="8202ee42-d3d5-4d91-a55f-078e4d6c02e8" data-execution_count="194">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> imread(<span class="st">'/content/gdrive/MyDrive/CS445Final/20230830_16_50_56.png'</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(img, axes<span class="op">=</span><span class="st">'YX'</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>plot(X[<span class="op">-</span><span class="dv">1</span>], pred, a[<span class="op">-</span><span class="dv">1</span>], <span class="st">'Noise2Void Output'</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"INFERNECE ON NEW IMAGE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 41ms/step
MSE (denoised vs average):  0.69177</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CS445FinalNotebookApr30_files/figure-html/cell-29-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>INFERNECE ON NEW IMAGE</code></pre>
</div>
</div>
<p>Noise2Void is certainly the most difficult denoiser to implement that we have seen so far. It was very hard to get it working. There is patch creation (which you have to hand pick the size of), and a ton of hyperparameters, which I mostly had no what to set them as. The model itself would have been impossible for me to build, and the data loading is quite complex as well.</p>
<p>Even with the increased difficulty of understanding and implementing the model, I do believe this is something that is worthwile for people to look at, and is maybe something I will continue to look into. The idea is really smart and provides good results, epseicially on the training set.</p>
<p>In the last code cell, I tried inference on a new image, and it did not do great.</p>
<hr>
<hr>
</section>
<section id="some-other-noise2noise-variants" class="level3">
<h3 class="anchored" data-anchor-id="some-other-noise2noise-variants"><strong>Some other Noise2Noise Variants</strong></h3>
<p><em>Neighbor2Neighbor</em>: Loosens dataset constraint compared to Noise2Noise by only requiring one noisy image. An algorithm is used to seperate the image into 2 images for input and target. Then a Noise2Noise style network is trained with these new pairs. [Huang et al., 2021]</p>
<p><em>Noise2Fast</em>: Similar to Noise2Void (because masking), but is significantly lighter weight and faster. Also only requires one noisy image, not noisy pairs like Noise2Noise. This is a very popular framework. [Lequyer et a., 2022]</p>
<p><em>ZeroShot Noise2Noise</em> Continuation on Neighbor2Neighbor, emphasizing only using a single image for the entire learning process. [Mansour et al., 2023]</p>
<p>All of these variants are increasingly complex compared to Nosie2Noise. They all have their purpose and place, but for my purpose they are overly convoluded and complex, and provide very similar outputs to Noise2Noise from what I have seen.</p>
<hr>
<hr>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>There is some serious validitity in using traditional/filter denoisers compared to deep learning. Very often you are only going to access to a single noisy image and require an instant denoiser. Traditional methods offer simple, quick and intuitive solutions. There is no need to set out a validation set, worry about overfitting, build a complicated model or dump a bunch of resources into GPUs and training. I saw especially promising results for my test images with the gaussian filter. I was really impressed with the results and I am definitely going to bring this method up in the lab. This also tells me that a gaussian distribution is a good approximation of my noise.</p>
<p>A huge drawback I noticed with traditional denoisers was the amount of hyperparameters, and how impactful they were to the denoising. This tells me that the traditional methods are not very flexible and they feel kind of unprincipled. Also they usually assume a particular type of noise, where deep learning methods may not.</p>
<p>I do think that if you do have access to noisy image stacks, Noise2Noise is going to be your best bet. It reveals structure very well, is consistent and can handle all different types of noise. Once you understand how it works, it is fairly easy to implement and use. There are not too many hyperparamters to chose from which was a huge bonus for me.</p>
<p>I was slightly underwhelmed by Noise2Void. I know that it is powerful and has its uses, but it just feels so overly engineered. There is also an absolutely ridiculous amount of hyperparameters. This anomicity toward Noise2Noise probably stems from the fact that I struggled with the implementation and didnt get great results…</p>
<p>A real downside of deep learning solutions is the huge leap in complexity. Not only does it require a ton of computational power, but it is also conceptually very difficult. This is a real downside for many people. Many different labs who are not expereinced in machine learning are looking for denoising. I find it hard to beleive that they would be able to implement most of these deep learning solutions.</p>
<hr>
<hr>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>[Krull et al., 2019] Krull, A., Buchholz, T.-O., &amp; Jug, F. (2019). Noise2Void - Learning Denoising from Single Noisy Images. arXiv.</p>
<p>[Lehtinen et al., 2018] Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., &amp; Aila, T. (2018). Noise2Noise: Learning Image Restoration without Clean Data. Proceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80.</p>
<p>[Ronneberger et al., 2015] Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany.</p>
<p>[Mansour et al., 2023] Mansour, Y., &amp; Heckel, R. (2023). Zero-Shot Noise2Noise: Efficient Image Denoising without any Data. arXiv</p>
<p>[Lequyer et a., 2022] Lequyer, J., Philip, R., Sharma, A., &amp; Pelletier, L. (2022). Noise2Fast: Fast Self-Supervised Single Image Blind Denoising. Nat Mach Intell, 4, 953-963. arXiv.</p>
<p>[Huang et al., 2021] Huang, T., Li, S., Jia, X., Lu, H., &amp; Liu, J. (2021). Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images. arXiv.</p>
<p>[Fan et al., 2019] Fan, L., Zhang, F., Fan, H., &amp; Zhang, C. (2019). Brief review of image denoising techniques. Visual Computing for Industry, Biomedicine, and Art, 2(7).</p>
<p>https://www.youtube.com/watch?v=71wqPyapFGU&amp;t=997s</p>
<p>https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>