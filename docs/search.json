[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I am from Seattle, Washington. I got my undergraduate degree from Colorado State University, and am currently a graduate student at the University of Washington.\nI am very interested in all things data science, computer science, and statistics. I have a special interest in using my technical skills for research, public health and sports.\nMy experience is mostly research based. During my undergrad at Colorado State University, I engaged in two research projects. First, with the Department of Statistics, I focused on testing complex methods to limit confounding in spatial data, specifically Thin Plate Regression Splines. Next, with the College of Engineering, I worked in the fascinating field of image de-noising, where I researched and developed deep learning solutions to enhance image quality. Most recently I worked in more of an analyst role, again with the CSU Statistics department. I worked with spatial air pollution data, where I conducted analyses, wrangled data and created visualizations."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Noise2Noise Research\n\n\nInvestigating unsupervised deep learning image denoising methods - specifically Noise2Noise - for use in a CSU microscopy lab pipeline. Developing novel ways to validate and…\n\n\n\nSam Herold\n\n\nJun 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Statistics Research\n\n\nAnalyzing effectiveness of Thin Plate Regression Splines for controlling unobserved spatial confounding. Extending traditional work from linear, continous data to Poisson…\n\n\n\nSam Herold\n\n\nMay 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Cow Pregnancy Outcomes\n\n\nA project in collaboration with the Animal Science department at CSU.\n\n\n\nSam Herold\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebook: Analysis of Unsupervised Denoising Methods\n\n\nOverview of the current state of image denoising. Investigating traditional and deep learning methods.\n\n\n\nSam Herold\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaught or Not\n\n\nAn app to predict if a runner would be safe or out if they attempted to steal second in a given scenario.\n\n\n\nSam Herold\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebook: SVM from Scratch\n\n\nA gritty battle in linear algebra to implement kernel and multiclass functionality for an SVM by hand.\n\n\n\nSam Herold\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Summer-2023-Stats-Research/index.html",
    "href": "posts/Summer-2023-Stats-Research/index.html",
    "title": "Spatial Statistics Research",
    "section": "",
    "text": "Presenting my research at the CSU Celebration of Undergraduate Research and Creativity\n\n\n\n\n\nA closer look at my research poster\n\n\n\n\nBackground\nSpatial confounding occurs when spatially correlated factors affect both the exposure and the outcome, making it difficult to isolate the true effect of the exposure. Spatial confounding is particularly relevant in environmental and epidemiological studies. Imagine a study examining the impact of air pollution on health outcomes, such as respiratory diseases. In a region with high air pollution, you might observe higher rates of respiratory illnesses. However, socioeconomic factors like income are also very correlated with air pollution, so income would confound the association between air pollution and respiratory diseases. The type of confounding we specifically want to address is unobserved spatial confounding, which just means we do not have collected data on the confounder.\n\n\n\n\nExample of confounding represented in a DAG\n\n\n\nThin Plate Regression Splines adjust for unknown spatial structure. TPRS use a mathematical process to create many spatial surfaces with varying structure, which are added to the model. These surfaces are combined to try to represent the confounding surface. TPRS flexibly model spatial variation in the outcome (dependent) variable. They can be useful to isolate the desired relationship in spatial data with a suspected confounder.\n\n\n\n\nDifferent TPRS. Early TPRS try to capture general trends, while later ones try to represent detail.\n\n\n\n\n\nContribution\nIt is known that TPRS work on linearly related data. My task was to investigate their efficacy on count data. It’s unclear how they might work: TPRS are traditionally designed for continuous data, there are alot of zeros in count data and the confounder is no longer linearly seperable from the theoretical model. We suspected that issues would arise. I created simulation scripts to test to see if TPRS could do a good job at limiting spatial confounding for Poisson data. We tested many different scenarios, including different surface shapes, different glms, multiple predictors, etc. Here is an example of a simulated spatial surface and a script to test the efficacy of TPRS on linear data:\n\n\n\n\nExample predictor and confounder surfaces. These are created through a random gaussian process, hence ‘gp.’ These surfaces are then used for simulation.\n\n\n\n\n\n\nExample simulation script for linear related TPRS.\n\n\n\n\n\nResults\nFail! TPRS do not show promise of being able to reduce spatial confounding in count data in ANY of our configurations. We believe the suspicions that we had above were true, but there is still work to be done on finding exactly why these don’t seem to work, and how to maybe develop a solution. Here is an example of one of our results.\n\n\nYour browser does not support iframes. You can download the file directly &lt;a href=\"docs/PoissonSimulationResults.html\"&gt;here&lt;/a&gt;."
  },
  {
    "objectID": "posts/SVM-By-Hand/index.html",
    "href": "posts/SVM-By-Hand/index.html",
    "title": "SVM by Hand",
    "section": "",
    "text": "An SVM out of the box can only separate linearly and only supports binary classification. This can be relieved with kernels and with the OneVsAll strategy. I implemented this functionality by hand on top of raw SVM code."
  },
  {
    "objectID": "posts/Caught-or-Not/index.html",
    "href": "posts/Caught-or-Not/index.html",
    "title": "Caught or Not",
    "section": "",
    "text": "Follow this link: https://caughtornot-6j6bascecfcd6rjstukygz.streamlit.app"
  },
  {
    "objectID": "posts/Cow-Preg-Outcomes/index.html",
    "href": "posts/Cow-Preg-Outcomes/index.html",
    "title": "Predicting Cow Pregnancy Outcomes",
    "section": "",
    "text": "Dr. Pablo Pinedo and Dr. Manriquez Alvarez of the CSU Department of Animal Sciences have a lot of data on cow pregnancies - health scores, milk produced, previous pregnancies, etc. They want to be able to predict the probability of a cow losing their pregnancy. Being able to accurately predict this would be extremely helpful for farmers everywhere and would be a huge step in precision farming. Below is a paper outlining the project. Further work to be done is to create an app for ease of use."
  },
  {
    "objectID": "posts/N2N-research/index.html",
    "href": "posts/N2N-research/index.html",
    "title": "Noise2Noise Research",
    "section": "",
    "text": "Presenting at the CSU CURC with my advisor Dr. Jesse Wilson\n\n\n\n\n\nA closer look\n\n\n\n\nBackground\nDr. Jesse Wilson and his lab produce molecular-level biological images. These images contain noise due to the nature of light and the equipment. A clean image can never be produced by the method. The objective of this research is to investigate ways to de-noise these images quickly, cheaply, and without damaging the sample.\nDeep learning methods have proven very effective in the image de-noising world. Expectantly, a model would be trained on noisy inputs and clean targets, and the model would learn how to go from noisy to clean. However, in many situations clean images are not available, as is ours. Luckily, Noise2Noise was developed, which does not require noisy images. In Noise2Noise, we simply replace the clean targets with another noisy instance of the same scene. The model performs as well as if there were clean targets. Read more at https://arxiv.org/abs/1803.04189.\n\n\n\n\nNoise2Noise is trained with a U-Net architecture. Above is a U-Net with noisy inputs and noisy targets\n\n\n\n\n\nContribution\nAn issue with Noise2Noise is that it’s impossible to tell how good you model is actually performing because we have don’t have any clean images to compare the results too. We address this problem with this research.\nWe developed a novel way to validate predictions without access to clean images. Essentially, we over-train a Noise2Noise model to get the cleanest possible image, then use those as ground truth/comparison images in a separate, second model. The poster above has more information. We are currently in the process of writing a manuscript to try to publish our work.\n\n\nGallery\n\n\n\nResults from experiments with simulated data.\n\n\n\n\n\nResults from experiments with real lab data.\n\n\n\n\n\nUNet implementation\n\n\n\n\n\nPerformance Graph"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam Herold",
    "section": "",
    "text": "Sam Herold is a graduate student at the University of Washington pursuing a M.S. in Biostatistics.\n\nEducation\nUniversity of Washington | Seattle, WA\nMaster of Science in Biostatistics\nColorado State University | Fort Collins, CO\nBachelor of Science in Data Science/Computer Science\n\n\nExperience\nResearch Assistant | CSU - Department of Statistics | Summer 2024\nUndergraduate Researcher | CSU - Walter Scott Jr. College of Engineering | Fall 2023 - Summer 2024\nUndergraduate Researcher | CSU - Department of Statistics | Summer 2023"
  },
  {
    "objectID": "posts/CS445FinalNotebook-checkpoint.html",
    "href": "posts/CS445FinalNotebook-checkpoint.html",
    "title": "Analysis of Unsupervised Denoising Methods",
    "section": "",
    "text": "Samuel Herold"
  },
  {
    "objectID": "posts/CS445FinalNotebook-checkpoint.html#introduction",
    "href": "posts/CS445FinalNotebook-checkpoint.html#introduction",
    "title": "Analysis of Unsupervised Denoising Methods",
    "section": "Introduction",
    "text": "Introduction\n\nAbstract\nImage denoising is a classic problem in computer vision and image processing. The goal is simple: to remove noise from a noisy image and recover the underlying image. The problem statement is often written as x ̂=x+n, where x ̂ is the observed image, n is additive noise and x is the ground truth image that we hope to recover. Often times, we do not have access to any x (ground truth) images, making the problem unsupervised. There are many ways people have attempted to do unsupervised denoising, and I will be exploring the most popular in this paper.\n\n\nBackground\nNoisy images are very prevalent in engineering, biology, and many other areas, making denoising very important and heavily researched. I was introduced to the world of image denoising during an undergraduate research project in the Department of Electrical Engineering, with Dr. Jesse Wilson and his lab. Their new laser scan technology produces biomedical images that contain a significant amount of noise. Their images will also always contain noise (due to the electronics, the nature of light, laser exposure limits, etc.), meaning there is no access to any ground truth images ever. This led to questions about unsupervised denoising options.\nThe labs current approach is stack averaging (taking multiple images of the same thing, then averaging them). This method is consistent, but costly, slow, and can damage the sample. This begged for an instant denoising method. My research task was to investigate the deep learning Noise2Noise framework. I will discuss this method later on in this paper.\nMy goal with this paper is to investigate unsupervised denoisers, both traditional and in deep learning, and use some of them on the biomedical data from the lab.\n\n\nDataset Notes\nI will be trying some of these methods on the noisy images collected from Dr. Jesse Wilson’s lab. These are a collection of 128x128 grayscale images, who’s pixel values are centered around zero, not between 0 and 255. We also believe the noise on the images is additive and gaussian.\n\n\nMethods and Frameworks\nThe first methods I will be looking at do not involve machine learning, so I will not go into as much detail as I will with Noise2Noise and its variants.\n\n#dependencies \nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\nfrom scipy.signal import wiener\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, lr_scheduler\n\nimport io\nfrom nbformat import current\nimport glob\n\n\n#load data\nX = []\npath = \"2023-08-30-FullDataSet\"\nfor image_class in os.listdir(path):\n    sameBaseImages = []\n    image_path = os.path.join(path, image_class)\n    img = np.array(Image.open(image_path))\n    X.append(img)\n\n\ndef plot(original, denoised, title):\n    plt.figure(figsize=(5, 5))\n    plt.subplot(121)\n    plt.imshow(original)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.subplot(122)\n    plt.imshow(denoised)\n    plt.title(title)\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "posts/CS445FinalNotebookApr30.html",
    "href": "posts/CS445FinalNotebookApr30.html",
    "title": "Notebook: Analysis of Unsupervised Denoising Methods",
    "section": "",
    "text": "Introduction\n\nAbstract\nImage denoising is a classic problem in computer vision and image processing. The goal is simple: to remove noise from a noisy image and recover the underlying image. The problem statement is often written as x ̂=x+n, where x ̂ is the observed image, n is additive noise and x is the ground truth image that we hope to recover. Often times, we do not have access to any x (ground truth) images, making the problem unsupervised. There are many ways people have attempted to do unsupervised denoising, and I will be exploring the most popular in this paper.\n\n\nBackground\nNoisy images are very prevalent in engineering, biology, and many other areas, making denoising very important and heavily researched. I was introduced to the world of image denoising during an undergraduate research project in the Department of Electrical Engineering, with Dr. Jesse Wilson and his lab. Their new laser scan technology produces biomedical images that contain a significant amount of noise. Their images will always contain noise (due to the electronics, the nature of light, laser exposure limits, etc.), meaning there is no access to any ground truth images ever. This led to questions about unsupervised denoising options.\nThe labs current approach is stack averaging (taking multiple images of the same thing, then averaging them). This method is consistent, but costly, slow, and can damage the sample. This begged for an instant denoising method. My research task was to investigate the deep learning Noise2Noise framework. I will discuss this method later on in this paper.\nMy goal with this paper is to investigate unsupervised denoisers, both traditional and in deep learning, and try them on some biomedical data from the lab.\n\n\nDataset Notes\nI will be trying some of these methods on the noisy images collected from Dr. Jesse Wilson’s lab. These are collected in stacks (collections of image with the same underlying image, but with different noise instances). They are 128x128 grayscale images, who’s pixel values are centered around zero, not between 0 and 255. We also believe the noise on the images is additive and gaussian. The noise level is very high on these images.\nFor understanding the output of the different denoisers, we will compare their outpurs to the stack averages, both visually and using MSE. The stack average is certainly not the ground truth, but the best that we can do and shows us at least some of the true underlying structure. The MSE definetely does not make or break the ability of the denoiser, it is just gives us some kind of slight reference.\n\n\nLoad dependencies, load data, and create helper methods\n\n#dependencies\nfrom PIL import Image\nimport numpy as np\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\nfrom scipy.signal import wiener\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, lr_scheduler\n\nimport io\nimport tensorflow as tf\nimport n2v\nfrom n2v.models import N2VConfig, N2V\nfrom csbdeep.utils import plot_history\nfrom n2v.utils.n2v_utils import manipulate_val_data\nfrom n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n\n\n#load\nX = []\na = [] #averages\npath = \"/content/gdrive/MyDrive/CS445Final/2023-08-30-FullDataSet\"\nXFull = []\n\n#loop through folder containing tifs\nfor image_class in os.listdir(path):\n      average = np.zeros([128,128])\n      sameBaseImages = []\n      #open tif file\n      image_path = os.path.join(path, image_class)\n      tif = Image.open(image_path)\n      X.append(np.array(tif))#just add one from each stack\n      #loads images for stack average\n      for i in range(tif.n_frames):\n          tif.seek(i)\n          img = np.array(tif)\n          XFull.append(img)\n          average += img\n\n      a.append(average/tif.n_frames)\n\n\nplt.subplot(121)\nplt.axis('off')\nplt.imshow(X[0])\nplt.title('Noisy')\nplt.subplot(122)\nplt.axis('off')\nplt.title('Stack Average')\nplt.imshow(a[0])\nplt.show()\nprint('Example of noisy image and its corresponding stack average, the closest to ground truth that we can get.  ')\n\n\n\n\n\n\n\n\nExample of noisy image and its corresponding stack average, the closest to ground truth that we can get.  \n\n\n\ndef plot(original, denoised,average, title):\n    print(\"MSE (denoised vs average): \" , round(np.mean((denoised - average)**2),5))\n    plt.figure(figsize=(10, 5))\n    plt.subplot(131)\n    plt.imshow(original)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.subplot(132)\n    plt.imshow(denoised)\n    plt.title(title)\n    plt.axis('off')\n    plt.subplot(133)\n    plt.imshow(average)\n    plt.title('Average Stack')\n    plt.axis('off')\n    plt.show()\n    plt.show()\n\n\n\n\n\n\nInvestigating Denoisers\n\nLinear Filtering\n[Fan et al., 2019]\nLinear filtering is the most straightforward way to denoise. This method involves sliding a moving window or kernel over each pixel in the image. Some operation is done on that window (i.e. mean, median), and the center pixel is replaced with the outcome of that operation. All of these filters have hyperparameters that you can tune for your images. In the following examples, I tuned the hyperparamters by hand until the outputs were as optimal as they could be.\nMean Filter: Take the mean of every surrounding pixel in the kernel and replace the center pixel with the mean.\nMedian Filter: Take the median of every surrounding pixel in the kernel and replace the center pixel with the mean.\nGaussian Filter: Uses a gaussian kernel for convolution. Uses a wieghted average of the kernel that is determined by the gaussian distribution. Smooths the image well (think of gaussian blur from photoshop).\nWeiner Filter: The wiener filter uses the frequency domain. It is complicated and uses Fourier Transforms.\n\nMean Filtering\n\nkernel = (6, 6)\nfiltered = ndimage.uniform_filter(X[0], size=kernel)\nplot(X[0], filtered, a[0],  'Mean Filtered Image')\n\nfiltered = ndimage.uniform_filter(X[1], size=kernel)\nplot(X[1], filtered, a[1], 'Mean Filtered Image')\n\nMSE (denoised vs average):  0.00546\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  0.00617\n\n\n\n\n\n\n\n\n\n\n\nMedian Filtering\n\nfiltered = ndimage.median_filter(X[0], size=4)\nplot(X[0], filtered, a[0],'Median Filtered Image')\n\nfiltered = ndimage.median_filter(X[1], size=4)\nplot(X[1], filtered, a[1],'Median Filtered Image')\n\nMSE (denoised vs average):  0.00756\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  0.00852\n\n\n\n\n\n\n\n\n\n\n\nGaussian Filtering\n\nfiltered = ndimage.gaussian_filter(X[0], sigma = 1.5)\nplot(X[0], filtered, a[0],'Gaussian Filtered Image')\n\nfiltered = ndimage.gaussian_filter(X[1], sigma = 1.5)\nplot(X[1], filtered, a[1],'Gaussian Filtered Image')\n\nMSE (denoised vs average):  0.00504\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  0.00579\n\n\n\n\n\n\n\n\n\n\n\nWiener Filtering\n\nfiltered = wiener(X[0],noise = 5)\nplot(X[0], filtered,a[0], 'Wiener Filtered Image')\n\nfiltered = wiener(X[1],noise = 5)\nplot(X[1], filtered, a[1],'Wiener Filtered Image')\n\nMSE (denoised vs average):  0.00765\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  0.00858\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimple to understand.\nSimple to implement.\nOnly need one image.\nSensitive to hyperparameters.\nVersatile.\n\n\n\nCons:\n\nPoor performance.\nOnly uses local information.\nOnly can use one image even if others are available.\nBad with edges.\n\nThe gaussian filter looks very promising. I will comment more on this in the conclusion.\n\n\n\n\n\nOther Filters\n[Fan et al., 2019]\nThese methods up the complexity and use information from the whole image. This increases performance and edge preservation. Also, these methods require that I put the images between 1 and 255. This isnt so much a downside, but rather an inconvienience as it changes the scale of the MSE.\nNon-Local Means: Uses patches of similar makeup from other parts of the image for averaging a window. This average is weighted by how similar the image patches are.\nBilateral Filtering: Uses a weighted average where spatial distance and pixel intensity are considered in weighting.\n\nNon-Local Means\n\nnormalized = ((X[0]  - X[0] .min()) / (X[0] .max() - X[0] .min()) * 255).astype(np.uint8) #we need to normalize for cv2 to be able to function\nave = ((a[0]  - a[0] .min()) / (a[0] .max() - a[0] .min()) * 255).astype(np.uint8) # also need to do this for averages\nNL = cv2.fastNlMeansDenoising(normalized, None, h=18, templateWindowSize=9, searchWindowSize=100)\nplot(X[0], NL, ave, 'Wiener Filtered Image')\n\nnormalized = ((X[1]  - X[1] .min()) / (X[1] .max() - X[1] .min()) * 255).astype(np.uint8)\nave = ((a[1]  - a[1] .min()) / (a[1] .max() - a[1] .min()) * 255).astype(np.uint8)\nNL = cv2.fastNlMeansDenoising(normalized, None, h=18, templateWindowSize=9, searchWindowSize=100)\nplot(X[1], NL, ave,'Wiener Filtered Image')\n\nMSE (denoised vs average):  97.04028\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  89.83875\n\n\n\n\n\n\n\n\n\n\n\nBilateral filtering\n\nnormalized = ((X[0]  - X[0] .min()) / (X[0] .max() - X[0] .min()) * 255).astype(np.uint8)\nbilateral = cv2.bilateralFilter(normalized, d=4, sigmaColor=75, sigmaSpace=75)\nave = ((a[0]  - a[0] .min()) / (a[0] .max() - a[0] .min()) * 255).astype(np.uint8)\nplot(X[0], bilateral, ave,'Wiener Filtered Image')\n\nnormalized = ((X[1]  - X[1] .min()) / (X[1] .max() - X[1] .min()) * 255).astype(np.uint8)\nbilateral = cv2.bilateralFilter(normalized, d=6, sigmaColor=75, sigmaSpace=75)\nave = ((a[1]  - a[1] .min()) / (a[1] .max() - a[1] .min()) * 255).astype(np.uint8)\nplot(X[1], bilateral, ave, 'Wiener Filtered Image')\n\nMSE (denoised vs average):  95.36255\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  87.04791\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimple to implement.\nIncreased complexity over simple filtering.\nRequire only one image.\nVersatile.\nCan pick up on detail.\n\n\n\nCons:\n\nDecently complex to understand.\nVery sensitive to its parameters.\nStruggles with edge preservation.\nMight create blurring/artifacts.\nDefinetly does not smooth the image.\n\n\n\n\n\n\nBM3D\n‘Block Matching 3D Filtering’ is a complex, state of the art denoising technique. This technique builds upon non-local means, in the sense that the algorithm groups similar patches using a block matching algorithm. These groups are then filtered an aggregated to produce the final image. BM3D is widely recognized to be one of the best options for denoising. It is easily avaible in the ‘bm3d’ package:\n\n!pip install bm3d;\n\nCollecting bm3d\n  Downloading bm3d-4.0.1-py3-none-any.whl (10.0 kB)\nCollecting bm4d&gt;=4.2.3 (from bm3d)\n  Downloading bm4d-4.2.3-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 5.8 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bm3d) (1.25.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bm3d) (1.11.4)\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from bm4d&gt;=4.2.3-&gt;bm3d) (1.6.0)\nInstalling collected packages: bm4d, bm3d\nSuccessfully installed bm3d-4.0.1 bm4d-4.2.3\n\n\n\nimport bm3d\nfrom skimage import io, img_as_float32\n\nThe main hyperparameter for bm3d is sigma: this is supposed to be the standard deviation of the noise. I could not figure out how to find this with no ground truth, so I tried many different sigmas to see which one had the visually best result. The resulting sigma was 45.\n\ns = 45\n\nimage = io.imread('/content/gdrive/MyDrive/CS445Final/Png2/20230830_15_40_14.png', as_gray = True)\nbm3dout = bm3d.bm3d(image, sigma_psd = s , stage_arg = bm3d.BM3DStages.ALL_STAGES)\nave = ((a[0]  - a[0] .min()) / (a[0] .max() - a[0] .min()) * 255).astype(np.uint8)\nplot(X[0],bm3dout, ave, 'BM3D Filtered Image')\n\nimage = io.imread('/content/gdrive/MyDrive/CS445Final/Png2/20230830_16_01_28.png', as_gray = True)\nbm3dout = bm3d.bm3d(image, sigma_psd = s, stage_arg = bm3d.BM3DStages.ALL_STAGES)\nave = ((a[1]  - a[1] .min()) / (a[1] .max() - a[1] .min()) * 255).astype(np.uint8)\nplot(X[1],bm3dout,ave, 'BM3D Filtered Image')\n\nimage = io.imread('/content/gdrive/MyDrive/CS445Final/Png2/20230830_16_01_54.png', as_gray = True)\nbm3dout = bm3d.bm3d(image, sigma_psd = s, stage_arg = bm3d.BM3DStages.ALL_STAGES)\nave = ((a[2]  - a[2] .min()) / (a[2] .max() - a[2] .min()) * 255).astype(np.uint8)\nplot(X[2],bm3dout, ave, 'BM3D Filtered Image')\n\nMSE (denoised vs average):  2422.69757\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  1621.71947\n\n\n\n\n\n\n\n\n\nMSE (denoised vs average):  1216.8787\n\n\n\n\n\n\n\n\n\nBM3D does not seem to be the most effective for this particular problem. I think visually it gives some decent results, but wierdly the MSE is really large. It definetely makes the image smooth and ‘not noisy’, but it misses a ton of detail in exchange. Everything I read says that it is probably one if not the best option for denoising a single image, so I do not count it out.\nPros: - Good performance. - Easy to implement and use. - Uses non-local imformation. - Versatile and robust. - Only requires on image.\nCons: - Sensitive to hyperparameters.. - Can take a long time for some images. - May miss out on some fine detail structure. - Mainly good at denoising additive gaussian. - Not available on Mac Silicon (this was frustrating).\n\n\n\n\nNoise2Noise\n[Lehtinen et al., 2018]\nIt is seemingly obvious that a neural network could learn to denoise images if trained on noisy input images with clean targets (the network learns how to go from noisy to clean). Call this Noise2Clean. However, it is often difficult, costly or impossible to get clean image targets. Noise2Noise is a deep learning framework that uses only noisy images to denoise images, and rivals the performance of the aforementioned noisy to clean method. This framework was proposed in NVidias 2018 paper, “Noise2Noise: Learning Image Restoration without Clean Data.”\nNoise2Noise uses noisy images as both the model input and the target. Through an interesting realization in math, this is essentially the same as using a clean image target. The model will try to predict the noisy target and fail. The best that it can do (while still reducing training loss) is predict the underlying, clean image.\nAnother way to think about it is with gradients. The gradient does not point exactly to the clean image, but rather many gradients point to images that average to the underlying clean images. In other words, the average gradient points to the unobserved clean image. In theory, the model will converge to the expected value of the noisy image.\n\n\n\nimage-2.png\n\n\n[Lehtinen et al., 2018]\nThere are some serious benefits to using Noise2Noise. If you have noisy image pairs, this will likely give the best denoising that you can ask for. Unlike the denoisers seen before this, this model can use information from multiple images. Just based off that, it makes sense that this framework will outperform algorithms that use just one image.\nNoise2Noise uses a U-NET architecture. Essentially, U-NET is an image to image convolutional neural network. The inputs and outputs are both images.\n\n\n\nimage-3.png\n\n\n[Ronneberger et al., 2015]\nAs you can see, UNET gets its name from its shape. In essence, it squeezes images down (encoding) to extract information, then builds them back up (decoding) to predict the target. The encoder section is a series of convolutional and max-pooling layers, and the decoder is a series of up sampling layers. Another important aspect of UNET is its skip connections (arrows across the top). These basically give the decoder some information about the structure at different stages of encoding that it can use if needed. Learning about upsampling, maxpooling and skip connections were all new to me and are super cool.\n\nEncoder\n\nConvolutional layer(s) followed by max pooling layer\nExtracts and separates information\n\nDecoder\n\nConvolutional layer and transpose convolutional layer.\nUp samples feature maps and combines them to predict the final image.\n\nSkip Connections\n\nConnect layers of the the encoder to layers of the decoder\nProvide information that may be useful information from earlier layers.\nThis makes extra sense for Noise2Noise, as the target image is very similar to the input image.\n\n\nBelow is an implementation of the UNET (in pytorch) used in Noise2Noise. This was taken from the Noise2Noise github page. This is a fairly complex model as you can see.\n\n#Unet pulled from Noise2Noise Github Repo [Lehtinen et al., 2018], [Ronneberger et al., 2015]\nclass UNet(nn.Module):\n    \"\"\"Custom U-Net architecture for Noise2Noise (see Appendix, Table 2).\"\"\"\n\n    def __init__(self, in_channels=1, out_channels=1):\n        \"\"\"Initializes U-Net.\"\"\"\n\n        super(UNet, self).__init__()\n\n        # Layers: enc_conv0, enc_conv1, pool1\n        self._block1 = nn.Sequential(\n            nn.Conv2d(in_channels, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(48, 48, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n\n        # Layers: enc_conv(i), pool(i); i=2..5\n        self._block2 = nn.Sequential(\n            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n\n        # Layers: enc_conv6, upsample5\n        self._block3 = nn.Sequential(\n            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(48, 48, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode='nearest'))\n\n        # Layers: dec_conv5a, dec_conv5b, upsample4\n        self._block4 = nn.Sequential(\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode='nearest'))\n\n        # Layers: dec_deconv(i)a, dec_deconv(i)b, upsample(i-1); i=4..2\n        self._block5 = nn.Sequential(\n            nn.Conv2d(144, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode='nearest'))\n\n        # Layers: dec_conv1a, dec_conv1b, dec_conv1c,\n        self._block6 = nn.Sequential(\n            nn.Conv2d(96 + in_channels, 64, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, out_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.1))\n\n        # Initialize weights\n        self._init_weights()\n\n\n    def _init_weights(self):\n        \"\"\"Initializes weights using He et al. (2015).\"\"\"\n\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.data)\n                m.bias.data.zero_()\n\n\n    def forward(self, x):\n        \"\"\"Through encoder, then decoder by adding U-skip connections. \"\"\"\n\n        # Encoder\n        pool1 = self._block1(x)\n        pool2 = self._block2(pool1)\n        pool3 = self._block2(pool2)\n        pool4 = self._block2(pool3)\n        pool5 = self._block2(pool4)\n\n        # Decoder\n        upsample5 = self._block3(pool5)\n        concat5 = torch.cat((upsample5, pool4), dim=1)\n        upsample4 = self._block4(concat5)\n        concat4 = torch.cat((upsample4, pool3), dim=1)\n        upsample3 = self._block5(concat4)\n        concat3 = torch.cat((upsample3, pool2), dim=1)\n        upsample2 = self._block5(concat3)\n        concat2 = torch.cat((upsample2, pool1), dim=1)\n        upsample1 = self._block5(concat2)\n        concat1 = torch.cat((upsample1, x), dim=1)\n\n        # Final activation\n        return self._block6(concat1)\n\nNow I will create noisy inputs and targets from the .tif stacks I have. I will be using 4 images with 10 noisy instances each for testing. This is a realistic capture budget you might see in the field. I could do more, but I have limited access to gpus, and I want to see if this is realistic for someone to use in the field. .\n\npath = '/content/gdrive/MyDrive/CS445Final/2023-08-30-FullDataSet'\nX = []\ny = []\nfor ind, image_class in enumerate(os.listdir(path)):\n    sameBaseImages = []\n    image_path = os.path.join(path, image_class)\n    tif = Image.open(image_path)\n    for i in range(10): #only 10 instances per image.\n        tif.seek(i)\n        img = np.array(tif)\n        sameBaseImages.append(img)\n\n    #randomly match up different noise instances of the same image\n    shuffled_images = sameBaseImages.copy()\n    random.shuffle(shuffled_images)\n    X.append(shuffled_images)\n    y.append(sameBaseImages)\n    if ind == 3: #create capture budget of only 4 images.\n        break\n\nX = np.array([item for row in X for item in row]) #flatten\ny = np.array([item for row in y for item in row])\nplt.subplot(121)\nplt.imshow(X[0])\nplt.title(\"Input\")\nplt.subplot(122)\nplt.imshow(y[0])\nplt.title(\"Target\")\nX = torch.from_numpy(X)\nX = X.unsqueeze(1)\ny = torch.from_numpy(y)\ny = y.unsqueeze(1)\nprint(\"X and y shape: \" ,X.shape , y.shape)\n\nX and y shape:  torch.Size([40, 1, 128, 128]) torch.Size([40, 1, 128, 128])\n\n\n\n\n\n\n\n\n\nNow initialize and train. We will be using L2 loss (though you can chose other loss metrics), and Adam. An interesing aspect of Noise2Noise is that there is often no validation or test set because the training set output is actually what we want. The traning image outputs are denoised!\n\nmodel = UNet()\ncriterion = nn.MSELoss()\noptim = Adam(model.parameters())\nuse_cuda = False\nif torch.cuda.is_available():\n    use_cuda = True\n    model = model.cuda()\n    criterion = criterion.cuda()\n    X = X.cuda()\n    y = y.cuda()\n\nnb_epochs = 250\ndenoisedExamples = []\nlossTrace = []\n\nfor epoch in range(nb_epochs):\n\n    denoised = model(X)\n    denoisedExamples.append(denoised)\n\n    loss = criterion(denoised, y)\n\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n    lossTrace.append(loss.item())\n    if epoch % 10 == 0:\n      print('EPOCH {:d} / {:d}'.format(epoch + 1, nb_epochs) , \"Loss: \" , loss.item())\n\nprint(\"Training done.\")\n\nEPOCH 1 / 250 Loss:  0.29845336079597473\nEPOCH 11 / 250 Loss:  0.054240882396698\nEPOCH 21 / 250 Loss:  0.0514160580933094\nEPOCH 31 / 250 Loss:  0.048928435891866684\nEPOCH 41 / 250 Loss:  0.047724079340696335\nEPOCH 51 / 250 Loss:  0.0469043031334877\nEPOCH 61 / 250 Loss:  0.046384233981370926\nEPOCH 71 / 250 Loss:  0.04599473997950554\nEPOCH 81 / 250 Loss:  0.045712895691394806\nEPOCH 91 / 250 Loss:  0.04553460702300072\nEPOCH 101 / 250 Loss:  0.04540208354592323\nEPOCH 111 / 250 Loss:  0.04528817534446716\nEPOCH 121 / 250 Loss:  0.04519662261009216\nEPOCH 131 / 250 Loss:  0.045131005346775055\nEPOCH 141 / 250 Loss:  0.045083049684762955\nEPOCH 151 / 250 Loss:  0.045046743005514145\nEPOCH 161 / 250 Loss:  0.045003242790699005\nEPOCH 171 / 250 Loss:  0.04496356472373009\nEPOCH 181 / 250 Loss:  0.04492597654461861\nEPOCH 191 / 250 Loss:  0.044932108372449875\nEPOCH 201 / 250 Loss:  0.044871117919683456\nEPOCH 211 / 250 Loss:  0.04482579603791237\nEPOCH 221 / 250 Loss:  0.04478657618165016\nEPOCH 231 / 250 Loss:  0.04479914531111717\nEPOCH 241 / 250 Loss:  0.04471750184893608\nTraining done.\n\n\n\nplt.scatter(np.arange(nb_epochs), lossTrace)\nplt.title('Training Loss Trace')\nplt.xlabel('Epoch')\nplt.ylabel('MSE')\n#plt.ylim(0, .1)\nplt.show()\n\n\n\n\n\n\n\n\n\nrows, cols = (2,5)\nplt.figure(figsize=(10,4))\nfor i in range(nb_epochs):\n    if i % 25 == 0:\n        plt.subplot(rows, cols, i// 25 + 1)\n        plt.imshow(denoisedExamples[i].cpu().detach().numpy()[0].reshape(128,128))\n        plt.title(f'Epoch {i+1}')\n        plt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(a[0])\nplt.title('Average of Stack')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nNoise2Noise does amazing. There is clearly underlying stucture that the model is finding. Unless the model completely memorizes the training set and ends up predicting the noisy target exactly, mathematically it has to be revealing true structure.\nWith a capture budget of 10 noisy instances for 4 images (40 total images), we can get very good denoising results. We do not need to know the noise distribution, the model learns it for us.\nAn issue with this method is inference on new images. That is what some of my research has to deal with, and is not something that I want to get into in this project.\n\nPros:\n\nCan use information from multiple images\nExcellent denoising.\nVersatile on many types of noise.\nDoes not create artifacts or new structure (in most circumstances).\nVery few hyperparameters\n\n\n\nCons:\n\nComplex.\nTakes a good amount of time and is computationaly expensive.\nUsing this for inference is difficult.\nRequires noisy image pairs (this is often impossible).\n\nNoise2Noise provides confident predictions, is easy to use and understand (once you think about if for a while), and is hard to mess up. I will comment more on Noise2Noise is the conclusion.\n\n\n\n\n\nNoise2Void\n[Krull et al., 2019]\nAs I mentioned, Noise2Noise has been built upon alot. Noise2Void is one of the most popular contributions to Noise2Noise, and I want to explore it.\nThe basic premise of Noise2Void is that the ground truth has underlying structure, and noise does not. Therefore we can predict the ground truth using the surrounding pixels and we cannot predict noise from the surrounding pixels. This method takes advantage of patch creation, like many other denoisers. Noise2Void ‘blinds’ the middle pixel of a patch of pixels, and uses the rest of the surrounding pixels to predict that blinded pixel.\n\n\n\nimage.png\n\n\n[Krull et al., 2019]\nThe framework only requires one image (not pairs) for denoising because it creates patches out of the images and those become the inputs and targets.\nI will be using a tensor flow implementation for this because it was the only way I could find how to do it. I followed this tutorial https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb and still struggled because of all of the moving parts. The training for this framework is a bit more involved, with the creation of patches and blinding pixels.\n\n#Noise2Void tutorial taken from https://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb\ndatagen = N2V_DataGenerator()\nimgs = datagen.load_imgs_from_directory('/content/gdrive/MyDrive/CS445Final/Png2',filter='*.png',dims='XY')\nprint('shape of loaded images: ', imgs[0].shape)\n\nshape of loaded images:  (1, 128, 128, 1)\n\n\n\npatch_size = 32\npatch_shape = (patch_size,patch_size)\npatches = datagen.generate_patches_from_list(imgs, shape=patch_shape)\n\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\nGenerated patches: (128, 32, 32, 1)\n\n\n\nnp.max(patches[0])\n\n0.91764706\n\n\n\ntrain_val_split = int(patches.shape[0] * 0.8)\nX_patch = patches[:train_val_split]\nX_val = patches[train_val_split:]\n\nprint(X_patch.shape)\nprint(X_val.shape)\n\n(1843, 32, 32, 1)\n(461, 32, 32, 1)\n\n\n\ntrain_batch = 32\nconfig = N2VConfig(X_patch, unet_kern_size=3,\n                   unet_n_first=64, unet_n_depth=3, train_steps_per_epoch=int(X_patch.shape[0]/train_batch), train_epochs=100, train_loss='mse',\n                   batch_norm=True, train_batch_size=train_batch, n2v_perc_pix=0.198, n2v_patch_shape=(patch_size, patch_size),\n                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5, single_net_per_channel=False)\n\n\nmodel_name = 'n2vModel32'\nbasedir = '/content/gdrive/MyDrive/CS445Final'\nmodel = N2V(config, model_name, basedir=basedir)\n\nhistory = model.train(X_patch, X_val)\n\n/usr/local/lib/python3.10/dist-packages/n2v/models/n2v_standard.py:447: UserWarning: output path for model already exists, files may be overwritten: /content/gdrive/MyDrive/CS445Final/n2vModel32\n  warnings.warn(\n\n\n2 blind-spots will be generated per training patch of size (32, 32).\n\n\nPreparing validation data: 100%|██████████| 461/461 [00:00&lt;00:00, 7139.75it/s]\n\n\nEpoch 1/100\n\n\n\n\n\n 5/57 [=&gt;............................] - ETA: 1s - loss: 2.0214 - n2v_mse: 2.0214 - n2v_abs: 1.1068\n\n\nWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0163s). Check your callbacks.\n\n\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 10s 50ms/step - loss: 1.2380 - n2v_mse: 1.2380 - n2v_abs: 0.8616 - val_loss: 1.0978 - val_n2v_mse: 1.1045 - val_n2v_abs: 0.8492 - lr: 4.0000e-04\nEpoch 2/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 42ms/step - loss: 0.8504 - n2v_mse: 0.8504 - n2v_abs: 0.7049 - val_loss: 0.8798 - val_n2v_mse: 0.8823 - val_n2v_abs: 0.7305 - lr: 4.0000e-04\nEpoch 3/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 34ms/step - loss: 0.7648 - n2v_mse: 0.7648 - n2v_abs: 0.6723 - val_loss: 0.8481 - val_n2v_mse: 0.8522 - val_n2v_abs: 0.7116 - lr: 4.0000e-04\nEpoch 4/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7562 - n2v_mse: 0.7562 - n2v_abs: 0.6755 - val_loss: 0.8817 - val_n2v_mse: 0.8845 - val_n2v_abs: 0.7294 - lr: 4.0000e-04\nEpoch 5/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.7011 - n2v_mse: 0.7011 - n2v_abs: 0.6465 - val_loss: 0.7806 - val_n2v_mse: 0.7772 - val_n2v_abs: 0.6585 - lr: 4.0000e-04\nEpoch 6/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7267 - n2v_mse: 0.7267 - n2v_abs: 0.6553 - val_loss: 0.7924 - val_n2v_mse: 0.7890 - val_n2v_abs: 0.6711 - lr: 4.0000e-04\nEpoch 7/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7334 - n2v_mse: 0.7334 - n2v_abs: 0.6585 - val_loss: 0.8120 - val_n2v_mse: 0.8111 - val_n2v_abs: 0.6791 - lr: 4.0000e-04\nEpoch 8/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 35ms/step - loss: 0.7403 - n2v_mse: 0.7403 - n2v_abs: 0.6673 - val_loss: 0.7853 - val_n2v_mse: 0.7832 - val_n2v_abs: 0.6545 - lr: 4.0000e-04\nEpoch 9/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 42ms/step - loss: 0.6958 - n2v_mse: 0.6958 - n2v_abs: 0.6519 - val_loss: 0.7972 - val_n2v_mse: 0.7955 - val_n2v_abs: 0.6703 - lr: 4.0000e-04\nEpoch 10/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.7023 - n2v_mse: 0.7023 - n2v_abs: 0.6464 - val_loss: 0.7861 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6659 - lr: 4.0000e-04\nEpoch 11/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7464 - n2v_mse: 0.7464 - n2v_abs: 0.6638 - val_loss: 0.8975 - val_n2v_mse: 0.9165 - val_n2v_abs: 0.7041 - lr: 4.0000e-04\nEpoch 12/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7545 - n2v_mse: 0.7545 - n2v_abs: 0.6724 - val_loss: 0.8093 - val_n2v_mse: 0.8029 - val_n2v_abs: 0.6655 - lr: 4.0000e-04\nEpoch 13/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7051 - n2v_mse: 0.7051 - n2v_abs: 0.6418 - val_loss: 0.9975 - val_n2v_mse: 1.0156 - val_n2v_abs: 0.7214 - lr: 4.0000e-04\nEpoch 14/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7441 - n2v_mse: 0.7441 - n2v_abs: 0.6639 - val_loss: 0.8361 - val_n2v_mse: 0.8282 - val_n2v_abs: 0.6740 - lr: 4.0000e-04\nEpoch 15/100\n57/57 [==============================] - ETA: 0s - loss: 0.7297 - n2v_mse: 0.7297 - n2v_abs: 0.6520\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.00019999999494757503.\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 35ms/step - loss: 0.7297 - n2v_mse: 0.7297 - n2v_abs: 0.6520 - val_loss: 0.8968 - val_n2v_mse: 0.8907 - val_n2v_abs: 0.6887 - lr: 4.0000e-04\nEpoch 16/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.6787 - n2v_mse: 0.6787 - n2v_abs: 0.6377 - val_loss: 0.7861 - val_n2v_mse: 0.7850 - val_n2v_abs: 0.6583 - lr: 2.0000e-04\nEpoch 17/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7304 - n2v_mse: 0.7304 - n2v_abs: 0.6591 - val_loss: 0.7834 - val_n2v_mse: 0.7815 - val_n2v_abs: 0.6513 - lr: 2.0000e-04\nEpoch 18/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6944 - n2v_mse: 0.6944 - n2v_abs: 0.6444 - val_loss: 0.7926 - val_n2v_mse: 0.7897 - val_n2v_abs: 0.6616 - lr: 2.0000e-04\nEpoch 19/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 44ms/step - loss: 0.7156 - n2v_mse: 0.7156 - n2v_abs: 0.6450 - val_loss: 0.7760 - val_n2v_mse: 0.7750 - val_n2v_abs: 0.6539 - lr: 2.0000e-04\nEpoch 20/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6908 - n2v_mse: 0.6908 - n2v_abs: 0.6367 - val_loss: 0.7680 - val_n2v_mse: 0.7683 - val_n2v_abs: 0.6480 - lr: 2.0000e-04\nEpoch 21/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6974 - n2v_mse: 0.6974 - n2v_abs: 0.6381 - val_loss: 0.7815 - val_n2v_mse: 0.7801 - val_n2v_abs: 0.6540 - lr: 2.0000e-04\nEpoch 22/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 38ms/step - loss: 0.7034 - n2v_mse: 0.7034 - n2v_abs: 0.6403 - val_loss: 0.7691 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6511 - lr: 2.0000e-04\nEpoch 23/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 36ms/step - loss: 0.6924 - n2v_mse: 0.6924 - n2v_abs: 0.6440 - val_loss: 0.7710 - val_n2v_mse: 0.7707 - val_n2v_abs: 0.6520 - lr: 2.0000e-04\nEpoch 24/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7199 - n2v_mse: 0.7199 - n2v_abs: 0.6554 - val_loss: 0.7912 - val_n2v_mse: 0.7878 - val_n2v_abs: 0.6523 - lr: 2.0000e-04\nEpoch 25/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6844 - n2v_mse: 0.6844 - n2v_abs: 0.6372 - val_loss: 0.7967 - val_n2v_mse: 0.8029 - val_n2v_abs: 0.6640 - lr: 2.0000e-04\nEpoch 26/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.7252 - n2v_mse: 0.7252 - n2v_abs: 0.6519 - val_loss: 0.7870 - val_n2v_mse: 0.7870 - val_n2v_abs: 0.6566 - lr: 2.0000e-04\nEpoch 27/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.7058 - n2v_mse: 0.7058 - n2v_abs: 0.6417 - val_loss: 0.7950 - val_n2v_mse: 0.7926 - val_n2v_abs: 0.6671 - lr: 2.0000e-04\nEpoch 28/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7087 - n2v_mse: 0.7087 - n2v_abs: 0.6473 - val_loss: 0.8128 - val_n2v_mse: 0.8106 - val_n2v_abs: 0.6593 - lr: 2.0000e-04\nEpoch 29/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 35ms/step - loss: 0.7035 - n2v_mse: 0.7035 - n2v_abs: 0.6409 - val_loss: 0.7808 - val_n2v_mse: 0.7793 - val_n2v_abs: 0.6586 - lr: 2.0000e-04\nEpoch 30/100\n57/57 [==============================] - ETA: 0s - loss: 0.7311 - n2v_mse: 0.7311 - n2v_abs: 0.6555\nEpoch 30: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.7311 - n2v_mse: 0.7311 - n2v_abs: 0.6555 - val_loss: 0.9077 - val_n2v_mse: 0.9252 - val_n2v_abs: 0.7188 - lr: 2.0000e-04\nEpoch 31/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7306 - n2v_mse: 0.7306 - n2v_abs: 0.6554 - val_loss: 0.7851 - val_n2v_mse: 0.7793 - val_n2v_abs: 0.6535 - lr: 1.0000e-04\nEpoch 32/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.6730 - n2v_mse: 0.6730 - n2v_abs: 0.6306 - val_loss: 0.7857 - val_n2v_mse: 0.7812 - val_n2v_abs: 0.6494 - lr: 1.0000e-04\nEpoch 33/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.6890 - n2v_mse: 0.6890 - n2v_abs: 0.6480 - val_loss: 0.7715 - val_n2v_mse: 0.7763 - val_n2v_abs: 0.6528 - lr: 1.0000e-04\nEpoch 34/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6914 - n2v_mse: 0.6914 - n2v_abs: 0.6420 - val_loss: 0.7749 - val_n2v_mse: 0.7758 - val_n2v_abs: 0.6536 - lr: 1.0000e-04\nEpoch 35/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7093 - n2v_mse: 0.7093 - n2v_abs: 0.6408 - val_loss: 0.7970 - val_n2v_mse: 0.7926 - val_n2v_abs: 0.6643 - lr: 1.0000e-04\nEpoch 36/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.7217 - n2v_mse: 0.7217 - n2v_abs: 0.6568 - val_loss: 0.7807 - val_n2v_mse: 0.7782 - val_n2v_abs: 0.6536 - lr: 1.0000e-04\nEpoch 37/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 41ms/step - loss: 0.6887 - n2v_mse: 0.6887 - n2v_abs: 0.6415 - val_loss: 0.7845 - val_n2v_mse: 0.7857 - val_n2v_abs: 0.6603 - lr: 1.0000e-04\nEpoch 38/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7151 - n2v_mse: 0.7151 - n2v_abs: 0.6535 - val_loss: 0.7707 - val_n2v_mse: 0.7719 - val_n2v_abs: 0.6523 - lr: 1.0000e-04\nEpoch 39/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.6591 - n2v_mse: 0.6591 - n2v_abs: 0.6271 - val_loss: 0.7750 - val_n2v_mse: 0.7786 - val_n2v_abs: 0.6555 - lr: 1.0000e-04\nEpoch 40/100\n56/57 [============================&gt;.] - ETA: 0s - loss: 0.6680 - n2v_mse: 0.6680 - n2v_abs: 0.6305\nEpoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n3/3 [==============================] - 0s 4ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.6739 - n2v_mse: 0.6739 - n2v_abs: 0.6316 - val_loss: 0.7722 - val_n2v_mse: 0.7714 - val_n2v_abs: 0.6524 - lr: 1.0000e-04\nEpoch 41/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 3s 44ms/step - loss: 0.6916 - n2v_mse: 0.6916 - n2v_abs: 0.6379 - val_loss: 0.7656 - val_n2v_mse: 0.7646 - val_n2v_abs: 0.6490 - lr: 5.0000e-05\nEpoch 42/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6847 - n2v_mse: 0.6847 - n2v_abs: 0.6387 - val_loss: 0.7670 - val_n2v_mse: 0.7658 - val_n2v_abs: 0.6486 - lr: 5.0000e-05\nEpoch 43/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.7018 - n2v_mse: 0.7018 - n2v_abs: 0.6402 - val_loss: 0.7839 - val_n2v_mse: 0.7962 - val_n2v_abs: 0.6567 - lr: 5.0000e-05\nEpoch 44/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 38ms/step - loss: 0.6524 - n2v_mse: 0.6524 - n2v_abs: 0.6241 - val_loss: 0.7722 - val_n2v_mse: 0.7769 - val_n2v_abs: 0.6497 - lr: 5.0000e-05\nEpoch 45/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.7115 - n2v_mse: 0.7115 - n2v_abs: 0.6516 - val_loss: 0.7937 - val_n2v_mse: 0.8046 - val_n2v_abs: 0.6589 - lr: 5.0000e-05\nEpoch 46/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6811 - n2v_mse: 0.6811 - n2v_abs: 0.6325 - val_loss: 0.7839 - val_n2v_mse: 0.7812 - val_n2v_abs: 0.6509 - lr: 5.0000e-05\nEpoch 47/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6985 - n2v_mse: 0.6985 - n2v_abs: 0.6472 - val_loss: 0.7842 - val_n2v_mse: 0.7842 - val_n2v_abs: 0.6569 - lr: 5.0000e-05\nEpoch 48/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6868 - n2v_mse: 0.6868 - n2v_abs: 0.6385 - val_loss: 0.7881 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6524 - lr: 5.0000e-05\nEpoch 49/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7152 - n2v_mse: 0.7152 - n2v_abs: 0.6438 - val_loss: 0.7766 - val_n2v_mse: 0.7790 - val_n2v_abs: 0.6523 - lr: 5.0000e-05\nEpoch 50/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 35ms/step - loss: 0.6980 - n2v_mse: 0.6980 - n2v_abs: 0.6433 - val_loss: 0.7755 - val_n2v_mse: 0.7768 - val_n2v_abs: 0.6513 - lr: 5.0000e-05\nEpoch 51/100\n56/57 [============================&gt;.] - ETA: 0s - loss: 0.7021 - n2v_mse: 0.7021 - n2v_abs: 0.6382\nEpoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 42ms/step - loss: 0.6995 - n2v_mse: 0.6995 - n2v_abs: 0.6380 - val_loss: 0.7763 - val_n2v_mse: 0.7752 - val_n2v_abs: 0.6532 - lr: 5.0000e-05\nEpoch 52/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7246 - n2v_mse: 0.7246 - n2v_abs: 0.6502 - val_loss: 0.7772 - val_n2v_mse: 0.7757 - val_n2v_abs: 0.6527 - lr: 2.5000e-05\nEpoch 53/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6618 - n2v_mse: 0.6618 - n2v_abs: 0.6341 - val_loss: 0.7818 - val_n2v_mse: 0.7823 - val_n2v_abs: 0.6554 - lr: 2.5000e-05\nEpoch 54/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6962 - n2v_mse: 0.6962 - n2v_abs: 0.6420 - val_loss: 0.7719 - val_n2v_mse: 0.7729 - val_n2v_abs: 0.6510 - lr: 2.5000e-05\nEpoch 55/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 3s 47ms/step - loss: 0.7091 - n2v_mse: 0.7091 - n2v_abs: 0.6425 - val_loss: 0.7654 - val_n2v_mse: 0.7657 - val_n2v_abs: 0.6479 - lr: 2.5000e-05\nEpoch 56/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6999 - n2v_mse: 0.6999 - n2v_abs: 0.6449 - val_loss: 0.7723 - val_n2v_mse: 0.7689 - val_n2v_abs: 0.6497 - lr: 2.5000e-05\nEpoch 57/100\n3/3 [==============================] - 0s 10ms/step\n57/57 [==============================] - 2s 40ms/step - loss: 0.7027 - n2v_mse: 0.7027 - n2v_abs: 0.6471 - val_loss: 0.7787 - val_n2v_mse: 0.7739 - val_n2v_abs: 0.6501 - lr: 2.5000e-05\nEpoch 58/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 38ms/step - loss: 0.7022 - n2v_mse: 0.7022 - n2v_abs: 0.6393 - val_loss: 0.7762 - val_n2v_mse: 0.7718 - val_n2v_abs: 0.6500 - lr: 2.5000e-05\nEpoch 59/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6751 - n2v_mse: 0.6751 - n2v_abs: 0.6327 - val_loss: 0.7768 - val_n2v_mse: 0.7734 - val_n2v_abs: 0.6508 - lr: 2.5000e-05\nEpoch 60/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7271 - n2v_mse: 0.7271 - n2v_abs: 0.6578 - val_loss: 0.7822 - val_n2v_mse: 0.7769 - val_n2v_abs: 0.6517 - lr: 2.5000e-05\nEpoch 61/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6944 - n2v_mse: 0.6944 - n2v_abs: 0.6355 - val_loss: 0.7807 - val_n2v_mse: 0.7756 - val_n2v_abs: 0.6520 - lr: 2.5000e-05\nEpoch 62/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7125 - n2v_mse: 0.7125 - n2v_abs: 0.6567 - val_loss: 0.7727 - val_n2v_mse: 0.7697 - val_n2v_abs: 0.6497 - lr: 2.5000e-05\nEpoch 63/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7025 - n2v_mse: 0.7025 - n2v_abs: 0.6433 - val_loss: 0.7732 - val_n2v_mse: 0.7704 - val_n2v_abs: 0.6506 - lr: 2.5000e-05\nEpoch 64/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.7020 - n2v_mse: 0.7020 - n2v_abs: 0.6453 - val_loss: 0.7739 - val_n2v_mse: 0.7746 - val_n2v_abs: 0.6537 - lr: 2.5000e-05\nEpoch 65/100\n57/57 [==============================] - ETA: 0s - loss: 0.7049 - n2v_mse: 0.7049 - n2v_abs: 0.6479\nEpoch 65: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 40ms/step - loss: 0.7049 - n2v_mse: 0.7049 - n2v_abs: 0.6479 - val_loss: 0.7814 - val_n2v_mse: 0.7841 - val_n2v_abs: 0.6586 - lr: 2.5000e-05\nEpoch 66/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6902 - n2v_mse: 0.6902 - n2v_abs: 0.6410 - val_loss: 0.7746 - val_n2v_mse: 0.7735 - val_n2v_abs: 0.6529 - lr: 1.2500e-05\nEpoch 67/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6791 - n2v_mse: 0.6791 - n2v_abs: 0.6347 - val_loss: 0.7734 - val_n2v_mse: 0.7727 - val_n2v_abs: 0.6518 - lr: 1.2500e-05\nEpoch 68/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6996 - n2v_mse: 0.6996 - n2v_abs: 0.6412 - val_loss: 0.7699 - val_n2v_mse: 0.7683 - val_n2v_abs: 0.6485 - lr: 1.2500e-05\nEpoch 69/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6849 - n2v_mse: 0.6849 - n2v_abs: 0.6431 - val_loss: 0.7678 - val_n2v_mse: 0.7662 - val_n2v_abs: 0.6474 - lr: 1.2500e-05\nEpoch 70/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6889 - n2v_mse: 0.6889 - n2v_abs: 0.6347 - val_loss: 0.7678 - val_n2v_mse: 0.7664 - val_n2v_abs: 0.6478 - lr: 1.2500e-05\nEpoch 71/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.6556 - n2v_mse: 0.6556 - n2v_abs: 0.6218 - val_loss: 0.7693 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6495 - lr: 1.2500e-05\nEpoch 72/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 40ms/step - loss: 0.6963 - n2v_mse: 0.6963 - n2v_abs: 0.6397 - val_loss: 0.7698 - val_n2v_mse: 0.7695 - val_n2v_abs: 0.6504 - lr: 1.2500e-05\nEpoch 73/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6787 - n2v_mse: 0.6787 - n2v_abs: 0.6371 - val_loss: 0.7670 - val_n2v_mse: 0.7663 - val_n2v_abs: 0.6486 - lr: 1.2500e-05\nEpoch 74/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6911 - n2v_mse: 0.6911 - n2v_abs: 0.6462 - val_loss: 0.7691 - val_n2v_mse: 0.7687 - val_n2v_abs: 0.6501 - lr: 1.2500e-05\nEpoch 75/100\n57/57 [==============================] - ETA: 0s - loss: 0.6884 - n2v_mse: 0.6884 - n2v_abs: 0.6378\nEpoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6884 - n2v_mse: 0.6884 - n2v_abs: 0.6378 - val_loss: 0.7698 - val_n2v_mse: 0.7688 - val_n2v_abs: 0.6504 - lr: 1.2500e-05\nEpoch 76/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6933 - n2v_mse: 0.6933 - n2v_abs: 0.6390 - val_loss: 0.7708 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6500 - lr: 6.2500e-06\nEpoch 77/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6919 - n2v_mse: 0.6919 - n2v_abs: 0.6348 - val_loss: 0.7704 - val_n2v_mse: 0.7680 - val_n2v_abs: 0.6493 - lr: 6.2500e-06\nEpoch 78/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 40ms/step - loss: 0.6793 - n2v_mse: 0.6793 - n2v_abs: 0.6372 - val_loss: 0.7706 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6485 - lr: 6.2500e-06\nEpoch 79/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.6649 - n2v_mse: 0.6649 - n2v_abs: 0.6262 - val_loss: 0.7696 - val_n2v_mse: 0.7668 - val_n2v_abs: 0.6481 - lr: 6.2500e-06\nEpoch 80/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.7041 - n2v_mse: 0.7041 - n2v_abs: 0.6483 - val_loss: 0.7701 - val_n2v_mse: 0.7678 - val_n2v_abs: 0.6487 - lr: 6.2500e-06\nEpoch 81/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 30ms/step - loss: 0.7154 - n2v_mse: 0.7154 - n2v_abs: 0.6533 - val_loss: 0.7690 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6490 - lr: 6.2500e-06\nEpoch 82/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7028 - n2v_mse: 0.7028 - n2v_abs: 0.6440 - val_loss: 0.7686 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6486 - lr: 6.2500e-06\nEpoch 83/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6957 - n2v_mse: 0.6957 - n2v_abs: 0.6417 - val_loss: 0.7701 - val_n2v_mse: 0.7686 - val_n2v_abs: 0.6498 - lr: 6.2500e-06\nEpoch 84/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6887 - n2v_mse: 0.6887 - n2v_abs: 0.6347 - val_loss: 0.7708 - val_n2v_mse: 0.7690 - val_n2v_abs: 0.6499 - lr: 6.2500e-06\nEpoch 85/100\n56/57 [============================&gt;.] - ETA: 0s - loss: 0.6920 - n2v_mse: 0.6920 - n2v_abs: 0.6394\nEpoch 85: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n3/3 [==============================] - 0s 8ms/step\n57/57 [==============================] - 2s 37ms/step - loss: 0.6924 - n2v_mse: 0.6924 - n2v_abs: 0.6385 - val_loss: 0.7695 - val_n2v_mse: 0.7677 - val_n2v_abs: 0.6488 - lr: 6.2500e-06\nEpoch 86/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.6830 - n2v_mse: 0.6830 - n2v_abs: 0.6371 - val_loss: 0.7685 - val_n2v_mse: 0.7670 - val_n2v_abs: 0.6481 - lr: 3.1250e-06\nEpoch 87/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6937 - n2v_mse: 0.6937 - n2v_abs: 0.6415 - val_loss: 0.7685 - val_n2v_mse: 0.7670 - val_n2v_abs: 0.6481 - lr: 3.1250e-06\nEpoch 88/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6890 - n2v_mse: 0.6890 - n2v_abs: 0.6430 - val_loss: 0.7684 - val_n2v_mse: 0.7665 - val_n2v_abs: 0.6477 - lr: 3.1250e-06\nEpoch 89/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6909 - n2v_mse: 0.6909 - n2v_abs: 0.6375 - val_loss: 0.7692 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6483 - lr: 3.1250e-06\nEpoch 90/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 33ms/step - loss: 0.6661 - n2v_mse: 0.6661 - n2v_abs: 0.6334 - val_loss: 0.7696 - val_n2v_mse: 0.7676 - val_n2v_abs: 0.6487 - lr: 3.1250e-06\nEpoch 91/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.7051 - n2v_mse: 0.7051 - n2v_abs: 0.6480 - val_loss: 0.7694 - val_n2v_mse: 0.7674 - val_n2v_abs: 0.6487 - lr: 3.1250e-06\nEpoch 92/100\n3/3 [==============================] - 0s 9ms/step\n57/57 [==============================] - 2s 37ms/step - loss: 0.6599 - n2v_mse: 0.6599 - n2v_abs: 0.6272 - val_loss: 0.7692 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6486 - lr: 3.1250e-06\nEpoch 93/100\n3/3 [==============================] - 0s 8ms/step\n57/57 [==============================] - 2s 39ms/step - loss: 0.7332 - n2v_mse: 0.7332 - n2v_abs: 0.6577 - val_loss: 0.7687 - val_n2v_mse: 0.7667 - val_n2v_abs: 0.6481 - lr: 3.1250e-06\nEpoch 94/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6783 - n2v_mse: 0.6783 - n2v_abs: 0.6348 - val_loss: 0.7691 - val_n2v_mse: 0.7675 - val_n2v_abs: 0.6485 - lr: 3.1250e-06\nEpoch 95/100\n55/57 [===========================&gt;..] - ETA: 0s - loss: 0.6860 - n2v_mse: 0.6860 - n2v_abs: 0.6357\nEpoch 95: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6868 - n2v_mse: 0.6868 - n2v_abs: 0.6359 - val_loss: 0.7698 - val_n2v_mse: 0.7685 - val_n2v_abs: 0.6491 - lr: 3.1250e-06\nEpoch 96/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6953 - n2v_mse: 0.6953 - n2v_abs: 0.6423 - val_loss: 0.7693 - val_n2v_mse: 0.7680 - val_n2v_abs: 0.6490 - lr: 1.5625e-06\nEpoch 97/100\n3/3 [==============================] - 0s 6ms/step\n57/57 [==============================] - 2s 31ms/step - loss: 0.6853 - n2v_mse: 0.6853 - n2v_abs: 0.6327 - val_loss: 0.7690 - val_n2v_mse: 0.7677 - val_n2v_abs: 0.6487 - lr: 1.5625e-06\nEpoch 98/100\n3/3 [==============================] - 0s 5ms/step\n57/57 [==============================] - 2s 32ms/step - loss: 0.6939 - n2v_mse: 0.6939 - n2v_abs: 0.6413 - val_loss: 0.7683 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6484 - lr: 1.5625e-06\nEpoch 99/100\n3/3 [==============================] - 0s 9ms/step\n57/57 [==============================] - 2s 35ms/step - loss: 0.6810 - n2v_mse: 0.6810 - n2v_abs: 0.6378 - val_loss: 0.7684 - val_n2v_mse: 0.7672 - val_n2v_abs: 0.6485 - lr: 1.5625e-06\nEpoch 100/100\n3/3 [==============================] - 0s 7ms/step\n57/57 [==============================] - 2s 40ms/step - loss: 0.6949 - n2v_mse: 0.6949 - n2v_abs: 0.6387 - val_loss: 0.7685 - val_n2v_mse: 0.7671 - val_n2v_abs: 0.6485 - lr: 1.5625e-06\n\nLoading network weights from 'weights_best.h5'.\n\n\n\nplt.figure(figsize=(16,5))\nplot_history(history,['loss','val_loss']);\n\n\n\n\n\n\n\n\n\nmodel_name = 'n2vModel32'\nbasedir = '/content/gdrive/MyDrive/CS445Final'\nmodel = N2V(config=None, name=model_name, basedir=basedir)\n\nfrom matplotlib.image import imread\nimg = imread('/content/gdrive/MyDrive/CS445Final/Png2/20230830_15_40_14.png')\n\npred = model.predict(img, axes='YX')\nplot(X[0], pred, a[0], 'Noise2Void Output')\n\nLoading network weights from 'weights_best.h5'.\n1/1 [==============================] - 0s 299ms/step\nMSE (denoised vs average):  1.03049\n\n\n\n\n\n\n\n\n\n\nimg = imread('/content/gdrive/MyDrive/CS445Final/20230830_16_50_56.png')\n\npred = model.predict(img, axes='YX')\nplot(X[-1], pred, a[-1], 'Noise2Void Output')\nprint(\"INFERNECE ON NEW IMAGE\")\n\n1/1 [==============================] - 0s 41ms/step\nMSE (denoised vs average):  0.69177\n\n\n\n\n\n\n\n\n\nINFERNECE ON NEW IMAGE\n\n\nNoise2Void is certainly the most difficult denoiser to implement that we have seen so far. It was very hard to get it working. There is patch creation (which you have to hand pick the size of), and a ton of hyperparameters, which I mostly had no what to set them as. The model itself would have been impossible for me to build, and the data loading is quite complex as well.\nEven with the increased difficulty of understanding and implementing the model, I do believe this is something that is worthwile for people to look at, and is maybe something I will continue to look into. The idea is really smart and provides good results, epseicially on the training set.\nIn the last code cell, I tried inference on a new image, and it did not do great.\n\n\n\n\nSome other Noise2Noise Variants\nNeighbor2Neighbor: Loosens dataset constraint compared to Noise2Noise by only requiring one noisy image. An algorithm is used to seperate the image into 2 images for input and target. Then a Noise2Noise style network is trained with these new pairs. [Huang et al., 2021]\nNoise2Fast: Similar to Noise2Void (because masking), but is significantly lighter weight and faster. Also only requires one noisy image, not noisy pairs like Noise2Noise. This is a very popular framework. [Lequyer et a., 2022]\nZeroShot Noise2Noise Continuation on Neighbor2Neighbor, emphasizing only using a single image for the entire learning process. [Mansour et al., 2023]\nAll of these variants are increasingly complex compared to Nosie2Noise. They all have their purpose and place, but for my purpose they are overly convoluded and complex, and provide very similar outputs to Noise2Noise from what I have seen.\n\n\n\n\n\nConclusion\nThere is some serious validitity in using traditional/filter denoisers compared to deep learning. Very often you are only going to access to a single noisy image and require an instant denoiser. Traditional methods offer simple, quick and intuitive solutions. There is no need to set out a validation set, worry about overfitting, build a complicated model or dump a bunch of resources into GPUs and training. I saw especially promising results for my test images with the gaussian filter. I was really impressed with the results and I am definitely going to bring this method up in the lab. This also tells me that a gaussian distribution is a good approximation of my noise.\nA huge drawback I noticed with traditional denoisers was the amount of hyperparameters, and how impactful they were to the denoising. This tells me that the traditional methods are not very flexible and they feel kind of unprincipled. Also they usually assume a particular type of noise, where deep learning methods may not.\nI do think that if you do have access to noisy image stacks, Noise2Noise is going to be your best bet. It reveals structure very well, is consistent and can handle all different types of noise. Once you understand how it works, it is fairly easy to implement and use. There are not too many hyperparamters to chose from which was a huge bonus for me.\nI was slightly underwhelmed by Noise2Void. I know that it is powerful and has its uses, but it just feels so overly engineered. There is also an absolutely ridiculous amount of hyperparameters. This anomicity toward Noise2Noise probably stems from the fact that I struggled with the implementation and didnt get great results…\nA real downside of deep learning solutions is the huge leap in complexity. Not only does it require a ton of computational power, but it is also conceptually very difficult. This is a real downside for many people. Many different labs who are not expereinced in machine learning are looking for denoising. I find it hard to beleive that they would be able to implement most of these deep learning solutions.\n\n\n\n\nReferences\n[Krull et al., 2019] Krull, A., Buchholz, T.-O., & Jug, F. (2019). Noise2Void - Learning Denoising from Single Noisy Images. arXiv.\n[Lehtinen et al., 2018] Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., & Aila, T. (2018). Noise2Noise: Learning Image Restoration without Clean Data. Proceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80.\n[Ronneberger et al., 2015] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany.\n[Mansour et al., 2023] Mansour, Y., & Heckel, R. (2023). Zero-Shot Noise2Noise: Efficient Image Denoising without any Data. arXiv\n[Lequyer et a., 2022] Lequyer, J., Philip, R., Sharma, A., & Pelletier, L. (2022). Noise2Fast: Fast Self-Supervised Single Image Blind Denoising. Nat Mach Intell, 4, 953-963. arXiv.\n[Huang et al., 2021] Huang, T., Li, S., Jia, X., Lu, H., & Liu, J. (2021). Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images. arXiv.\n[Fan et al., 2019] Fan, L., Zhang, F., Fan, H., & Zhang, C. (2019). Brief review of image denoising techniques. Visual Computing for Industry, Biomedicine, and Art, 2(7).\nhttps://www.youtube.com/watch?v=71wqPyapFGU&t=997s\nhttps://github.com/bnsreenu/python_for_microscopists/blob/master/293_denoising_RGB_images_using_deep%20learning.ipynb"
  },
  {
    "objectID": "posts/SVM-from-scratch-notebook.html",
    "href": "posts/SVM-from-scratch-notebook.html",
    "title": "Notebook: SVM from Scratch",
    "section": "",
    "text": "Introduction\nWarning: This notebook is extremely dense, difficult to follow, and arranged poorly - but I am proud of it so I decided to post it. I will be rearranging it for better readability soon\nAn SVM out of the box can only separate linearly and only supports binary classification. This can be relieved with kernels and with the OneVsAll strategy. I implemented this functionality by hand on top of raw SVM code, then tested it on ecoli data. I also compared my implementation to the sklearn equivalent.\n\n\nDependencies and Data Cleaning\n\nfrom sklearn.datasets import fetch_openml\nimport time\nimport numpy as np\nfrom numpy import linalg as LA\nfrom sklearn.model_selection import train_test_split\nimport time\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import polynomial_kernel, linear_kernel\n\n\ndata = pd.read_fwf('ecoli/ecoli.data', header = None, ) \ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 336 entries, 0 to 335\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       336 non-null    object \n 1   1       336 non-null    float64\n 2   2       336 non-null    float64\n 3   3       336 non-null    float64\n 4   4       336 non-null    float64\n 5   5       336 non-null    float64\n 6   6       336 non-null    float64\n 7   7       336 non-null    float64\n 8   8       336 non-null    object \ndtypes: float64(7), object(2)\nmemory usage: 23.8+ KB\n\n\n\n#break into features and labels\nx = data.iloc[:,1:-1].to_numpy()\ny = data.iloc[:,-1].to_numpy()\n\n\n#encode all of the 'cp', 'imL', etc. stuff into numbers for classification\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\n#mappings and value counts\nprint(np.unique(y, return_counts=True))\ndic = dict(zip( le.transform(le.classes_), le.classes_))\ndic\n\n(array([0, 1, 2, 3, 4, 5, 6, 7]), array([  2,   2, 143,  77,   5,  35,  20,  52]))\n\n\n{0: 'L', 1: 'S', 2: 'cp', 3: 'im', 4: 'mL', 5: 'mU', 6: 'om', 7: 'pp'}\n\n\n\nprint(np.unique(y))\n\n[0 1 2 3 4 5 6 7]\n\n\n\nx.shape, y.shape\n\n((336, 7), (336,))\n\n\n\n\nFunctions\nWe will be changing the x_train and x_test data-sets later, so they are variables in the function\n\n#break dataset for OneVall. 'All' is -1, 'One' is 1\ndef breakDataSet(y_train, oneClass):\n    y_train_copy = y_train.copy()\n    \n    y_train_copy[y_train != oneClass] = -1\n    y_train_copy[y_train == oneClass] = 1\n    \n    return y_train_copy\n\n\n#function to get the svm weights and biases\ndef getSVMs(x_train, y_train): \n    \n    \n    #make a flexible dataframe to store the wieghts and biases depending on the input shape. first column for \n    #storing index, everything else execpt lass for weights per feature, last one for bias\n    n_columns = x_train.shape[1] + 1\n    column_names = [f'Weight_{i}' for i in range(n_columns)]\n    df = pd.DataFrame(0, index=range(8), columns=column_names)\n    df = df.rename(columns={column_names[-1]: 'Bias'})\n\n\n    #print(df)\n    \n    #loop through all 8 classes and make a OneVall for each\n\n    for c in range(len(np.unique(y))):\n        \n\n        ydual_train  = breakDataSet(y_train.copy(), oneClass = c)\n        xdual_train = x_train.copy()\n\n        N = len(ydual_train)\n        XPY = xdual_train.copy()\n        for i in range(N):\n            if ydual_train[i]==-1:\n                XPY[i,:] =-1 * xdual_train[i,:]  \n                \n           \n        A = np.matmul(XPY,XPY.transpose())\n\n        AT = A.copy().transpose()\n\n        YM = np.outer(ydual_train[1:],ydual_train[1:])\n        AY = np.outer(A[0,1:],ydual_train[1:])\n        YA = np.outer(ydual_train[1:],A[0,1:])\n\n\n        Y0S = ydual_train[0]**2\n        M = AT[1:,1:] + A[0,0]*YM/Y0S - AY/ydual_train[0] - YA/ydual_train[0]\n\n\n        b = np.zeros(N-1)\n        b = 1 - ydual_train[1:]/ydual_train[0]\n\n\n        aw = np.zeros(N)\n        for i in range(2,N):\n            aw[i] = (1-ydual_train[i]/ydual_train[0])/(A[i,i] + A[0,0]*ydual_train[i]**2/ydual_train[0]**2 \n                                                   - 2*A[0,i]*ydual_train[i]/ydual_train[0])\n\n        aw[0] = -sum(ydual_train[1:]*aw[1:])/ydual_train[0]\n\n\n\n        YA   = ydual_train*aw\n\n        wght = sum(xdual_train * YA[:,None])\n        b =sum(ydual_train - np.matmul(xdual_train,wght))/N\n        \n\n       \n        df.iloc[c,0:-1] = wght\n        df.iloc[c,-1] = b\n            \n    return df\n\n\n\n# run through each of the 8 classifiers and pick the class of the classifier that \n# is the most confident in the 'One' not the 'All'\ndef getAccuracy(x_test, y_test, SVM):\n    testSetAcc = []\n    for j in range(x_test.shape[0]):\n        results = []\n        #loop through all 8 available SVM's\n        for i in range(8):\n            #W*features + bias\n            results.append((SVM.iloc[i,:-1].values@x_test[j]) + SVM.iloc[i,-1])\n        #pick most confident result, store true if it is the same as the actual output\n        #argmax is the most confident result. A positive number means pick the one, negative means pick the rest. \n        #so the highest positive number is the most confident 'one' result. \n        \n        testSetAcc.append(np.argmax(results) == y_test[j])\n        \n    #print(testSetAcc)\n    if len(testSetAcc) == 0:\n        return 0.0\n        \n    return sum(testSetAcc)/len(testSetAcc)\n\n\n#see accuracy for each specific class\n#should correspond to number of training examples\ndef getAccuracyPerNumber(x_test, y_test, SVM):\n    for i in range(8):\n        #split data into only x and y of one class\n        yonlyI  = y_test[y_test == i]\n        xonlyI  = x_test[y_test == i]\n        acc = getAccuracy(xonlyI, yonlyI, SVM)\n        print(\"Accuracy for \" , i , \" or \" , dic[i] , \" : \" , acc)\n\n\n\nMultiClass SVM’s\n\n#get the train test split\n#we will be changing x_train and x_test for the kernels\nx_train, x_test, y_train, y_test = train_test_split(x,y,train_size = .8,shuffle = True, random_state = 4)\nx.shape\n\n(336, 7)\n\n\n\nSklearn implementations\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n\n#Linear SVM\nlinearSVC = LinearSVC(dual=\"auto\")\nlinearSVC.fit(x_train,y_train)\ny_pred = linearSVC.predict(x_test)\n\"sklean Linear SVM Accuracy: \" , accuracy_score(y_pred,y_test)\n\n('sklean Linear SVM Accuracy: ', 0.9411764705882353)\n\n\n\n#Linear Kernel SVM\nlinearKernelSVC = SVC(kernel = \"linear\")\nlinearKernelSVC.fit(x_train,y_train)\ny_pred = linearKernelSVC.predict(x_test)\n\"sklean Linear Kernel SVM Accuracy: \" , accuracy_score(y_pred,y_test)\n\n('sklean Linear Kernel SVM Accuracy: ', 0.8382352941176471)\n\n\n\n#Polynomial Kernel SVM\npolySVC = SVC(kernel = \"poly\")\npolySVC.fit(x_train,y_train)\ny_pred = polySVC.predict(x_test)\n\"sklean Polynomial SVM Accuracy: \" , accuracy_score(y_pred,y_test)\n\n('sklean Polynomial SVM Accuracy: ', 0.8823529411764706)\n\n\n\n#RBF Kernel SVM\nrbfSVC = SVC(kernel = \"rbf\")\nrbfSVC.fit(x_train,y_train)\ny_pred = rbfSVC.predict(x_test)\n\"sklean RBF SVM Accuracy: \" , accuracy_score(y_pred,y_test)\n\n('sklean RBF SVM Accuracy: ', 0.8970588235294118)\n\n\n\n\nLinear (No Kernel) OneVsAll SVM\nDo not change x_train or x_test\n\nlinearSVM = getSVMs(x_train = x_train, y_train = y_train)\nlinearSVM \n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nBias\n\n\n\n\n0\n0.652312\n-1.943609\n3.244773e+00\n1.531159e+00\n-0.087257\n0.987371\n0.218332\n-3.284968\n\n\n1\n3.037805\n-5.788647\n-8.881784e-16\n-8.881784e-16\n1.392839\n2.789961\n-0.208726\n-1.625561\n\n\n2\n-189.074352\n-305.870475\n-2.056133e-13\n-3.610445e-13\n-12.150815\n-132.073554\n-109.956294\n379.137984\n\n\n3\n-47.842855\n-116.962305\n2.079584e+00\n2.797762e-14\n37.575416\n107.329767\n87.197460\n-35.081053\n\n\n4\n1.535861\n-5.719501\n1.089051e+01\n8.881784e-16\n1.658700\n1.569109\n-6.278016\n-2.754340\n\n\n5\n23.890656\n-68.690367\n2.519990e+00\n-2.176037e-14\n16.202489\n56.151497\n45.912457\n-39.212604\n\n\n6\n8.893138\n-11.033733\n1.243450e-14\n-3.552714e-15\n53.116386\n-9.926145\n-52.436130\n4.703137\n\n\n7\n198.907436\n516.008637\n-1.873485e+01\n-1.531159e+00\n-97.707757\n-26.828006\n35.550916\n-307.882595\n\n\n\n\n\n\n\n\nacc = getAccuracy(x_test, y_test, SVM = linearSVM)\n\"Overall accuracy: \" , acc\n\n('Overall accuracy: ', 0.7647058823529411)\n\n\n\ngetAccuracyPerNumber(x_test, y_test, SVM = linearSVM)\n\nAccuracy for  0  or  L  :  0.0\nAccuracy for  1  or  S  :  0.0\nAccuracy for  2  or  cp  :  1.0\nAccuracy for  3  or  im  :  0.6111111111111112\nAccuracy for  4  or  mL  :  0.0\nAccuracy for  5  or  mU  :  0.0\nAccuracy for  6  or  om  :  0.0\nAccuracy for  7  or  pp  :  1.0\n\n\n\nprint(\"Total Value Counts: \" , np.unique(y, return_counts=True))\nprint(\"Train Value Counts: \" , np.unique(y_train, return_counts=True))\nprint(\"Test Value Counts: \" , np.unique(y_test, return_counts=True))\n\nTotal Value Counts:  (array([0, 1, 2, 3, 4, 5, 6, 7]), array([  2,   2, 143,  77,   5,  35,  20,  52]))\nTrain Value Counts:  (array([0, 1, 2, 3, 4, 5, 6, 7]), array([  2,   2, 109,  59,   5,  29,  17,  45]))\nTest Value Counts:  (array([2, 3, 5, 6, 7]), array([34, 18,  6,  3,  7]))\n\n\n\n\nLinear Kernel OneVsAll SVM\nHere is a sanity check. Doing the nested for loops and dot product is the same as using ‘@’, aka matmul, which is also the same result as sklearn. The three cells below are equal. I will be going forward with ‘@’ method.\n\nm1,_ = x_train.shape\nm2,_ = x_train.shape\nK = np.zeros((m1, m2))\nfor i in range(m1):\n    for j in range(m2):\n        K[i,j] = (np.dot(x_train[i,:], x_train[j,:]))\nK\n\narray([[2.2258, 2.1549, 1.8836, ..., 2.224 , 2.3089, 1.4967],\n       [2.1549, 2.1061, 1.7873, ..., 2.1159, 2.1993, 1.4239],\n       [1.8836, 1.7873, 1.7736, ..., 2.0285, 2.1597, 1.3861],\n       ...,\n       [2.224 , 2.1159, 2.0285, ..., 2.3671, 2.4829, 1.6044],\n       [2.3089, 2.1993, 2.1597, ..., 2.4829, 2.6862, 1.632 ],\n       [1.4967, 1.4239, 1.3861, ..., 1.6044, 1.632 , 1.1844]])\n\n\n\n#sklean implementaion\nlinear_kernel(x_train, x_train)\n\narray([[2.2258, 2.1549, 1.8836, ..., 2.224 , 2.3089, 1.4967],\n       [2.1549, 2.1061, 1.7873, ..., 2.1159, 2.1993, 1.4239],\n       [1.8836, 1.7873, 1.7736, ..., 2.0285, 2.1597, 1.3861],\n       ...,\n       [2.224 , 2.1159, 2.0285, ..., 2.3671, 2.4829, 1.6044],\n       [2.3089, 2.1993, 2.1597, ..., 2.4829, 2.6862, 1.632 ],\n       [1.4967, 1.4239, 1.3861, ..., 1.6044, 1.632 , 1.1844]])\n\n\n\n#this is the easiest\nx_train_lk = (x_train@x_train.T)\nx_train_lk\n\narray([[2.2258, 2.1549, 1.8836, ..., 2.224 , 2.3089, 1.4967],\n       [2.1549, 2.1061, 1.7873, ..., 2.1159, 2.1993, 1.4239],\n       [1.8836, 1.7873, 1.7736, ..., 2.0285, 2.1597, 1.3861],\n       ...,\n       [2.224 , 2.1159, 2.0285, ..., 2.3671, 2.4829, 1.6044],\n       [2.3089, 2.1993, 2.1597, ..., 2.4829, 2.6862, 1.632 ],\n       [1.4967, 1.4239, 1.3861, ..., 1.6044, 1.632 , 1.1844]])\n\n\n\nx_test_lk = (x_test@x_train.T)\nx_test_lk.shape\n\n(68, 268)\n\n\n\nlinearKernelSVM = getSVMs(x_train_lk, y_train)\nlinearKernelSVM \n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_259\nWeight_260\nWeight_261\nWeight_262\nWeight_263\nWeight_264\nWeight_265\nWeight_266\nWeight_267\nBias\n\n\n\n\n0\n0.029816\n0.031341\n0.040736\n0.033559\n0.030333\n0.038119\n0.074498\n0.032969\n0.033898\n0.045377\n...\n0.035622\n0.043069\n0.044931\n0.041545\n0.032067\n0.039507\n0.034011\n0.042333\n0.035511\n-19.493451\n\n\n1\n-0.128826\n-0.095660\n-0.020059\n-0.031667\n-0.140254\n-0.074763\n0.018640\n-0.112745\n-0.084766\n0.083664\n...\n-0.110066\n0.054653\n0.071271\n-0.024115\n-0.066290\n-0.027101\n-0.075397\n0.044607\n-0.097745\n19.601696\n\n\n2\n-2.325591\n-2.298186\n-1.547406\n-2.027124\n-1.505314\n-1.079832\n-2.110807\n-1.858849\n-2.388795\n-2.091542\n...\n-1.363291\n-1.101895\n-2.233211\n-1.326405\n-2.104018\n-1.569929\n-2.004983\n-2.020710\n-1.116802\n881.333004\n\n\n3\n-0.852687\n-1.614870\n2.031921\n-1.340171\n0.350807\n0.815637\n2.861968\n0.588308\n-1.113975\n2.036954\n...\n-0.194748\n-0.121520\n1.673075\n0.370304\n-1.189052\n-0.095279\n1.509874\n2.306719\n0.908610\n-339.834706\n\n\n4\n-0.029030\n0.045244\n0.157004\n0.203149\n0.095817\n0.210922\n0.623288\n0.039999\n0.010446\n0.158640\n...\n0.133733\n0.381164\n0.123264\n0.211067\n0.112517\n0.188817\n0.042357\n0.134176\n0.191198\n-72.285431\n\n\n5\n0.112292\n-0.088166\n1.268413\n-0.034959\n0.064405\n0.415518\n2.050423\n0.497145\n0.269832\n1.922934\n...\n0.088798\n0.481926\n1.841427\n0.634497\n0.018704\n0.501776\n0.978662\n1.787581\n0.313826\n-373.791961\n\n\n6\n-0.424177\n-0.123569\n-0.169247\n1.169128\n0.478533\n0.035312\n-1.023782\n-0.288359\n-0.846068\n-0.182589\n...\n-0.472083\n1.254460\n-0.555692\n-0.392064\n0.629220\n-0.144211\n0.091844\n0.077451\n0.355122\n45.004395\n\n\n7\n3.618203\n4.143866\n-1.761363\n2.028084\n0.625673\n-0.360913\n-2.494228\n1.101533\n4.119428\n-1.973437\n...\n1.882034\n-0.991856\n-0.965066\n0.485172\n2.566852\n1.106421\n-0.576369\n-2.372157\n-0.589721\n-146.533546\n\n\n\n\n8 rows × 269 columns\n\n\n\n\nacc = getAccuracy(x_test_lk, y_test, SVM = linearKernelSVM )\n\"Overall accuracy: \" , acc\n\n('Overall accuracy: ', 0.75)\n\n\n\ngetAccuracyPerNumber(x_test_lk, y_test, SVM = linearKernelSVM )\n\nAccuracy for  0  or  L  :  0.0\nAccuracy for  1  or  S  :  0.0\nAccuracy for  2  or  cp  :  1.0\nAccuracy for  3  or  im  :  0.6666666666666666\nAccuracy for  4  or  mL  :  0.0\nAccuracy for  5  or  mU  :  0.0\nAccuracy for  6  or  om  :  0.0\nAccuracy for  7  or  pp  :  0.7142857142857143\n\n\n\n\nPolynomial Kernel OneVsAll SVM\n\nx_train_pk = (x_train@x_train.T + 1) ** 2\nx_train_pk\n\narray([[10.40578564,  9.95339401,  8.31514896, ..., 10.394176  ,\n        10.94881921,  6.23351089],\n       [ 9.95339401,  9.64785721,  7.76904129, ...,  9.70883281,\n        10.23552049,  5.87529121],\n       [ 8.31514896,  7.76904129,  7.69285696, ...,  9.17181225,\n         9.98370409,  5.69347321],\n       ...,\n       [10.394176  ,  9.70883281,  9.17181225, ..., 11.33736241,\n        12.13059241,  6.78289936],\n       [10.94881921, 10.23552049,  9.98370409, ..., 12.13059241,\n        13.58807044,  6.927424  ],\n       [ 6.23351089,  5.87529121,  5.69347321, ...,  6.78289936,\n         6.927424  ,  4.77160336]])\n\n\n\n#sanity check\npolynomial_kernel(x_train, x_train, coef0 = 1,degree = 2, gamma = 1)\n\narray([[10.40578564,  9.95339401,  8.31514896, ..., 10.394176  ,\n        10.94881921,  6.23351089],\n       [ 9.95339401,  9.64785721,  7.76904129, ...,  9.70883281,\n        10.23552049,  5.87529121],\n       [ 8.31514896,  7.76904129,  7.69285696, ...,  9.17181225,\n         9.98370409,  5.69347321],\n       ...,\n       [10.394176  ,  9.70883281,  9.17181225, ..., 11.33736241,\n        12.13059241,  6.78289936],\n       [10.94881921, 10.23552049,  9.98370409, ..., 12.13059241,\n        13.58807044,  6.927424  ],\n       [ 6.23351089,  5.87529121,  5.69347321, ...,  6.78289936,\n         6.927424  ,  4.77160336]])\n\n\n\nx_test_pk = (x_test@x_train.T + 1) ** 2\nx_test_pk.shape\n\n(68, 268)\n\n\n\npolyKernelSVM = getSVMs(x_train_pk, y_train)\npolyKernelSVM \n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_259\nWeight_260\nWeight_261\nWeight_262\nWeight_263\nWeight_264\nWeight_265\nWeight_266\nWeight_267\nBias\n\n\n\n\n0\n0.004834\n0.005039\n0.005986\n0.005297\n0.004272\n0.004829\n0.013938\n0.004978\n0.005503\n0.007628\n...\n0.004672\n0.005670\n0.007641\n0.005535\n0.005094\n0.005575\n0.005472\n0.007058\n0.004584\n-13.824352\n\n\n1\n-0.023621\n-0.017087\n-0.004547\n-0.005689\n-0.021920\n-0.010647\n0.001381\n-0.019562\n-0.015748\n0.013672\n...\n-0.015720\n0.007285\n0.011544\n-0.004101\n-0.011885\n-0.004748\n-0.014880\n0.006353\n-0.014056\n15.978626\n\n\n2\n-0.416950\n-0.402582\n-0.253623\n-0.348199\n-0.236895\n-0.151190\n-0.424119\n-0.313249\n-0.423700\n-0.389532\n...\n-0.195676\n-0.157768\n-0.419470\n-0.193139\n-0.366234\n-0.242053\n-0.363583\n-0.376329\n-0.160401\n688.493775\n\n\n3\n-0.159119\n-0.271945\n0.281023\n-0.221829\n0.037008\n0.090878\n0.490914\n0.070131\n-0.200211\n0.314265\n...\n-0.035489\n-0.026978\n0.254283\n0.033847\n-0.203196\n-0.028990\n0.223788\n0.361466\n0.105949\n-224.619765\n\n\n4\n-0.004263\n0.007710\n0.024554\n0.033053\n0.014552\n0.027467\n0.120441\n0.006940\n0.002037\n0.028506\n...\n0.017913\n0.051007\n0.022459\n0.028652\n0.018709\n0.027283\n0.008252\n0.024390\n0.025677\n-51.248862\n\n\n5\n0.013673\n-0.015428\n0.171149\n-0.007165\n0.006301\n0.045628\n0.346046\n0.066663\n0.037082\n0.302391\n...\n0.008669\n0.054846\n0.292425\n0.074199\n0.000226\n0.061801\n0.145269\n0.279373\n0.034917\n-253.385481\n\n\n6\n-0.070384\n-0.019263\n-0.024850\n0.191273\n0.069667\n0.004393\n-0.187206\n-0.044716\n-0.139000\n-0.029472\n...\n-0.061981\n0.166707\n-0.094902\n-0.051840\n0.104079\n-0.020110\n0.016249\n0.015183\n0.046496\n42.392377\n\n\n7\n0.655830\n0.713557\n-0.199691\n0.353259\n0.127015\n-0.011356\n-0.361395\n0.228815\n0.734036\n-0.247458\n...\n0.277612\n-0.100768\n-0.073980\n0.106847\n0.453206\n0.201242\n-0.020567\n-0.317494\n-0.043166\n-209.786318\n\n\n\n\n8 rows × 269 columns\n\n\n\n\nacc = getAccuracy(x_test_pk, y_test, SVM = polyKernelSVM)\n\"Overall accuracy: \" , acc\n\n('Overall accuracy: ', 0.75)\n\n\n\ngetAccuracyPerNumber(x_test_pk, y_test, SVM = polyKernelSVM)\n\nAccuracy for  0  or  L  :  0.0\nAccuracy for  1  or  S  :  0.0\nAccuracy for  2  or  cp  :  1.0\nAccuracy for  3  or  im  :  0.6666666666666666\nAccuracy for  4  or  mL  :  0.0\nAccuracy for  5  or  mU  :  0.0\nAccuracy for  6  or  om  :  0.0\nAccuracy for  7  or  pp  :  0.7142857142857143\n\n\n\n\nRBF Kernel OneVsAll SVM\n\n#change x_TRAIN into the kernel form\ngamma = 5\nm1 = x_train.shape[0]\nm2 = x_train.shape[0]\nK = np.zeros((m1, m2))  \nfor i in range(m1):\n    for j in range(m2):\n        K[i,j] = np.exp( - gamma * np.linalg.norm(x_train[i,:] - x_train[j,:])**2 )\nx_train_g = K \nx_train_g.shape\n\n(268, 268)\n\n\n\n#change x_TEST into the kernel form\nm1 = x_test.shape[0]\nm2 = x_train.shape[0]\nK = np.zeros((m1, m2))  \nfor i in range(m1):\n    for j in range(m2):\n        K[i,j] = np.exp( -gamma * np.linalg.norm(x_test[i,:] - x_train[j,:])**2 )\nx_test_g = K \nx_test_g.shape\n\n(68, 268)\n\n\n\ngaussSVM = getSVMs(x_train_g, y_train)\ngaussSVM\n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_259\nWeight_260\nWeight_261\nWeight_262\nWeight_263\nWeight_264\nWeight_265\nWeight_266\nWeight_267\nBias\n\n\n\n\n0\n-0.103223\n-0.092350\n-0.027322\n-0.057581\n-0.029807\n-0.012262\n0.051829\n-0.069209\n-0.081645\n-0.009872\n...\n-0.023576\n-0.010042\n-0.011961\n-0.022334\n-0.079101\n-0.042691\n-0.045777\n-0.013807\n-0.012573\n1.827474\n\n\n1\n-0.227459\n-0.152850\n-0.008310\n-0.014010\n-0.069576\n-0.018224\n0.008834\n-0.142911\n-0.130679\n0.073483\n...\n-0.046448\n0.030229\n0.066294\n-0.008841\n-0.090179\n-0.017816\n-0.067931\n0.047155\n-0.023147\n1.008084\n\n\n2\n-4.606170\n-3.815017\n1.224969\n-1.362157\n1.652092\n3.377806\n-0.139220\n-0.917722\n-3.781187\n-0.633320\n...\n2.667629\n2.945758\n-0.885892\n2.841779\n-2.454242\n1.825746\n-1.390003\n-0.633337\n3.059048\n-45.821309\n\n\n3\n-2.704025\n-2.720102\n0.544612\n-1.627132\n-0.434572\n-0.050359\n0.385037\n-1.031915\n-2.237631\n0.771766\n...\n-0.463443\n-0.196975\n0.584650\n-0.349531\n-2.186835\n-0.924014\n0.187455\n0.972225\n-0.024639\n28.860400\n\n\n4\n-0.299173\n-0.257952\n-0.075215\n-0.133496\n-0.077349\n-0.027998\n0.027091\n-0.194469\n-0.236544\n-0.047653\n...\n-0.061333\n-0.011248\n-0.056205\n-0.053314\n-0.207097\n-0.104974\n-0.134204\n-0.055853\n-0.029051\n6.596430\n\n\n5\n-1.333802\n-1.309359\n0.138252\n-0.823313\n-0.405615\n-0.131105\n0.293040\n-0.710659\n-0.958108\n0.956391\n...\n-0.301839\n-0.110349\n0.919359\n-0.182641\n-1.095606\n-0.445006\n-0.053855\n0.834657\n-0.149837\n14.922934\n\n\n6\n-1.749565\n-1.191424\n-0.351199\n0.847945\n0.045767\n-0.029922\n-0.097058\n-1.011908\n-1.710756\n-0.277898\n...\n-0.291936\n0.378823\n-0.414643\n-0.265084\n-0.006951\n-0.436253\n-0.507447\n-0.230471\n0.044358\n22.060349\n\n\n7\n11.023417\n9.539053\n-1.445785\n3.169743\n-0.680940\n-3.107938\n-0.529552\n4.078794\n9.136550\n-0.832898\n...\n-1.479055\n-3.026196\n-0.201600\n-1.960034\n6.120009\n0.145008\n2.011762\n-0.920568\n-2.864159\n-35.454363\n\n\n\n\n8 rows × 269 columns\n\n\n\n\nacc = getAccuracy(x_test_g, y_test, SVM = gaussSVM)\n\"Overall accuracy: \" , acc\n\n('Overall accuracy: ', 0.7941176470588235)\n\n\n\ngetAccuracyPerNumber(x_test_g, y_test, SVM = gaussSVM)\n\nAccuracy for  0  or  L  :  0.0\nAccuracy for  1  or  S  :  0.0\nAccuracy for  2  or  cp  :  1.0\nAccuracy for  3  or  im  :  0.7222222222222222\nAccuracy for  4  or  mL  :  0.0\nAccuracy for  5  or  mU  :  0.0\nAccuracy for  6  or  om  :  0.0\nAccuracy for  7  or  pp  :  1.0\n\n\n\n\n\nBinary SVM’s, classifying 2 (‘cp’) and 3 (‘im’)\n\nHelper Functions\n\ndef breakDataSetBinary(x,y, classNeg, classPos):\n    y_copy = y[(y == classNeg) | (y == classPos)]\n    x_copy = x[(y == classNeg) | (y == classPos)]\n    \n    y_copy[y_copy == classNeg] = -1\n    y_copy[y_copy ==classPos] = 1\n    \n    return x_copy,y_copy\n    \n\n\n#function to get the svm weights and biases\ndef getSVMBinary(x_train, y_train): \n    \n    n_columns = x_train.shape[1] + 1\n    column_names = [f'Weight_{i}' for i in range(n_columns)]\n    df = pd.DataFrame(0, index=range(1), columns=column_names)\n    df = df.rename(columns={column_names[-1]: 'Bias'})\n    \n    ydual_train  = y_train.copy()\n    xdual_train = x_train.copy()\n\n    N = len(ydual_train)\n    XPY = xdual_train.copy()\n    for i in range(N):\n        if ydual_train[i]==-1:\n            XPY[i,:] =-1 * xdual_train[i,:]    \n    A = np.matmul(XPY,XPY.transpose())\n    AT = A.copy().transpose()\n    YM = np.outer(ydual_train[1:],ydual_train[1:])\n    AY = np.outer(A[0,1:],ydual_train[1:])\n    YA = np.outer(ydual_train[1:],A[0,1:])\n    Y0S = ydual_train[0]**2\n    M = AT[1:,1:] + A[0,0]*YM/Y0S - AY/ydual_train[0] - YA/ydual_train[0]\n    b = np.zeros(N-1)\n    b = 1 - ydual_train[1:]/ydual_train[0]\n    aw = np.zeros(N)\n    for i in range(2,N):\n        aw[i] = (1-ydual_train[i]/ydual_train[0])/(A[i,i] + A[0,0]*ydual_train[i]**2/ydual_train[0]**2 \n                                                   - 2*A[0,i]*ydual_train[i]/ydual_train[0])\n    aw[0] = -sum(ydual_train[1:]*aw[1:])/ydual_train[0]\n    YA   = ydual_train*aw\n\n    wght = sum(xdual_train * YA[:,None])\n    b =sum(ydual_train - np.matmul(xdual_train,wght))/N\n    \n    df.iloc[0,0:-1] = wght\n    df.iloc[0,-1] = b\n        \n    return df\n\n\ndef getBinaryAccuracy(x_test, y_test, SVM):\n    testSetAcc = []\n    for j in range(x_test.shape[0]):\n        pred = (SVM.iloc[0,:-1].values@x_test[j]) + SVM.iloc[0,-1] \n        \n        testSetAcc.append(np.sign([pred]) == y_test[j])\n    \n    return sum(testSetAcc)/len(testSetAcc)\n\n\n#make new x and y for whatever 2-class dataset we want\nbinX, binY = breakDataSetBinary(x,y, 2, 3)\nbinX.shape, binY.shape\n\n((220, 7), (220,))\n\n\n\nx_train_bin, x_test_bin, y_train_bin, y_test_bin = train_test_split(binX, binY, train_size = .8,shuffle = True, random_state = 4)\n\n\n\nSklearn Implementaion\n\n#Linear SVM\nlinearSVC = LinearSVC(dual=\"auto\")\nlinearSVC.fit(x_train_bin,y_train_bin)\ny_pred = linearSVC.predict(x_test_bin)\n\"sklean Linear SVM Accuracy: \" , accuracy_score(y_pred,y_test_bin)\n\n('sklean Linear SVM Accuracy: ', 0.9772727272727273)\n\n\n\n#Linear Kernel SVM\nlinearKernelSVC = SVC(kernel = \"linear\")\nlinearKernelSVC.fit(x_train_bin,y_train_bin)\ny_pred = linearKernelSVC.predict(x_test_bin)\n\"sklean Linear Kernel SVM Accuracy: \" , accuracy_score(y_pred,y_test_bin)\n\n('sklean Linear Kernel SVM Accuracy: ', 0.9772727272727273)\n\n\n\n#Polynomial Kernel SVM\npolySVC = SVC(kernel = \"poly\")\npolySVC.fit(x_train_bin,y_train_bin)\ny_pred = polySVC.predict(x_test_bin)\n\"sklean Polynomial SVM Accuracy: \" , accuracy_score(y_pred,y_test_bin)\n\n('sklean Polynomial SVM Accuracy: ', 0.9772727272727273)\n\n\n\n#RBF Kernel SVM\nrbfSVC = SVC(kernel = \"rbf\")\nrbfSVC.fit(x_train_bin,y_train_bin)\ny_pred = rbfSVC.predict(x_test_bin)\n\"sklean RBF SVM Accuracy: \" , accuracy_score(y_pred,y_test_bin)\n\n('sklean RBF SVM Accuracy: ', 0.9772727272727273)\n\n\n\n\nLinear (No Kernel) Binary SVM\n\nbinarySVM = getSVMBinary(x_train_bin, y_train_bin)\nbinarySVM\n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nBias\n\n\n\n\n0\n114.44046\n8.256893\n-2.020606e-14\n-4.507505e-14\n30.061676\n174.304006\n152.11576\n-226.904196\n\n\n\n\n\n\n\n\nacc = getBinaryAccuracy(x_test_bin, y_test_bin, binarySVM)[0]\n\"Accuracy: \" , acc\n\n('Accuracy: ', 0.9772727272727273)\n\n\n\n\nLinear Kernel Binary SVM\n\nx_train_binLin = (x_train_bin@x_train_bin.T +1)\n\n\nx_test_binLin = (x_test_bin@x_train_bin.T +1)\n\n\nbinaryLinSVM = getSVMBinary(x_train_binLin, y_train_bin)\nbinaryLinSVM\n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_167\nWeight_168\nWeight_169\nWeight_170\nWeight_171\nWeight_172\nWeight_173\nWeight_174\nWeight_175\nBias\n\n\n\n\n0\n2.776165\n1.568457\n1.127405\n1.184949\n1.25222\n1.156369\n1.512408\n1.243416\n2.637813\n0.976\n...\n1.310995\n2.269961\n1.40919\n1.086981\n1.39031\n2.036696\n1.216645\n2.397667\n1.052194\n-748.535817\n\n\n\n\n1 rows × 177 columns\n\n\n\n\nacc = getBinaryAccuracy(x_test_binLin, y_test_bin, binaryLinSVM)[0]\n\"Accuracy: \" , acc\n\n('Accuracy: ', 0.9545454545454546)\n\n\n\n\nPolynomial Kernel Binary SVM\n\nx_train_binPoly = (x_train_bin@x_train_bin.T +1 )**2\n\n\nx_test_binPoly = (x_test_bin@x_train_bin.T +1 )** 2\n\n\nbinaryPolySVM = getSVMBinary(x_train_binPoly, y_train_bin)\nbinaryPolySVM\n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_167\nWeight_168\nWeight_169\nWeight_170\nWeight_171\nWeight_172\nWeight_173\nWeight_174\nWeight_175\nBias\n\n\n\n\n0\n0.52576\n0.234171\n0.152427\n0.159653\n0.177088\n0.159331\n0.224659\n0.170257\n0.494367\n0.12192\n...\n0.186919\n0.400485\n0.202948\n0.14208\n0.199616\n0.3514\n0.166262\n0.433306\n0.140494\n-333.53882\n\n\n\n\n1 rows × 177 columns\n\n\n\n\nacc = getBinaryAccuracy(x_test_binPoly, y_test_bin, binaryPolySVM)[0]\n\"Accuracy: \" , acc\n\n('Accuracy: ', 1.0)\n\n\n\n\nRBF Kernel Binary SVM\n\n#change x_TRAIN into the kernel form\ngamma = 1\n\nm1 = x_train_bin.shape[0]\nm2 = x_train_bin.shape[0]\nK = np.zeros((m1, m2))  \nfor i in range(m1):\n    for j in range(m2):\n        K[i,j] = np.exp( -gamma * np.linalg.norm(x_train_bin[i,:] - x_train_bin[j,:])**2 )\n        \nx_train_binRBF = K \nx_train_binRBF.shape\n\n(176, 176)\n\n\n\n#change x_TEST into the kernel form\ngamma = 1\n\nm1 = x_test_bin.shape[0]\nm2 = x_train_bin.shape[0]\nK = np.zeros((m1, m2))  \nfor i in range(m1):\n    for j in range(m2):\n        K[i,j] = np.exp( -gamma * np.linalg.norm(x_test_bin[i,:] - x_train_bin[j,:])**2 )\n        \nx_test_binRBF = K \nx_test_binRBF.shape\n\n(44, 176)\n\n\n\nbinaryRBFSVM = getSVMBinary(x_train_binRBF, y_train_bin)\nbinaryRBFSVM\n\n\n\n\n\n\n\n\nWeight_0\nWeight_1\nWeight_2\nWeight_3\nWeight_4\nWeight_5\nWeight_6\nWeight_7\nWeight_8\nWeight_9\n...\nWeight_167\nWeight_168\nWeight_169\nWeight_170\nWeight_171\nWeight_172\nWeight_173\nWeight_174\nWeight_175\nBias\n\n\n\n\n0\n4.205387\n-2.243984\n-3.838294\n-3.877483\n-3.657845\n-3.933399\n-2.453572\n-3.638815\n3.42117\n-4.347771\n...\n-3.369824\n1.782538\n-2.968907\n-4.137613\n-3.104279\n0.423154\n-3.524469\n2.413333\n-4.1625\n253.240474\n\n\n\n\n1 rows × 177 columns\n\n\n\n\nacc = getBinaryAccuracy(x_test_binRBF, y_test_bin, binaryRBFSVM)[0]\n\"Accuracy: \" , acc\n\n('Accuracy: ', 0.9772727272727273)"
  }
]